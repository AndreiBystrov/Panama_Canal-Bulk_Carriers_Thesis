# -*- coding: utf-8 -*-
"""Statistical Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17-X0Mw_lDnofOWhcDV9Id0roKxWsHq9c
"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import matplotlib.ticker as mticker
import numpy as np
import seaborn as sns

"""Import Bulk PA clean dataframe"""

# Read Bulk_PA_original_data_copy from paruqet
Bulk_PA_original_data_copy = pd.read_parquet('Bulk_PA_original_data_copy.parquet')

# Read Bulk_PA_final from parquet
Bulk_PA_final = pd.read_parquet('Bulk_PA_final.parquet')

Bulk_PA_original_data_copy.info()

"""# **Holt-Winters decomposition & forecast**

## **Original Data** (not cleaned data - duplicate deleted)
"""

# Bulk_PA_original_data_copy to timestamp & datetime
Bulk_PA_original_data_copy['month'] = Bulk_PA_original_data_copy['month'].dt.to_timestamp()
Bulk_PA_original_data_copy['month'] = pd.to_datetime(Bulk_PA_original_data_copy['month'])

# Split Bulk_PA_original_data_copy for training & forecast - before & during El-Nino
Bulk_PA_original_data_copy_train = Bulk_PA_original_data_copy[(Bulk_PA_original_data_copy['month'] < '2023-06-01') & (Bulk_PA_original_data_copy['month'] >= '2019-01-01')]
Bulk_PA_original_data_copy_forecast = Bulk_PA_original_data_copy[Bulk_PA_original_data_copy['month'] >= '2023-06-01']

# Scatter plot of the count of month column in Bulk_PA_original_data_copy
monthly_counts = Bulk_PA_original_data_copy.groupby('month').size().reset_index(name='count')

# Filter data to be between 2019 and 2025
monthly_counts_filtered = monthly_counts[(monthly_counts['month'] >= '2019-01-01') & (monthly_counts['month'] <= '2025-01-01')]


# Create plot
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
plt.plot(monthly_counts_filtered['month'], monthly_counts_filtered['count'], marker='o')
plt.xlabel('Month')
plt.ylabel('Count')
plt.title('Monthly Transits (2019-2024)\n\nUnfiltered data')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Holt-Winters time-series decomposition

# Group by month to get the number of transits
monthly_transits = Bulk_PA_original_data_copy_train.groupby('month').size().rename("number_transits")
monthly_transits = monthly_transits.asfreq('MS')  # Ensure monthly frequency

# Plot the original series
monthly_transits.plot(title="Monthly Number of Transits", marker='o')
plt.ylabel("Transits")
plt.grid(True)
plt.show()

# Holt-Winters model (additive seasonality if there is clear monthly seasonality)
model = ExponentialSmoothing(
    monthly_transits,
    trend='add',
    damped_trend=True,
    seasonal='add',
    seasonal_periods=12,
    initialization_method='estimated'
)
fit = model.fit(optimized=True)

# Forecast the next 18 months
forecast = fit.forecast(18)

# Plot the results
plt.figure(figsize=(10, 5))
plt.plot(monthly_transits, label='Observed')
plt.plot(fit.fittedvalues, label='Fitted')
plt.plot(forecast, label='Forecast', marker='o')
plt.title('Holt-Winters Forecast of Monthly Transits')
plt.legend()
plt.grid(True)
plt.show()

# Print smoothing parameters
params = fit.params
print("alpha:", params.get("smoothing_level"))
print("beta :", params.get("smoothing_trend"))
print("gamma:", params.get("smoothing_seasonal"))
print("phi  :", params.get("damping_trend"))

print("\nInitial values:")
print(f"  Initial level:               {params.get('initial_level', 'N/A')}")
print(f"  Initial trend:               {params.get('initial_trend', 'N/A')}")
print(f"  Seasonal initial values:     {params.get('initial_seasons', 'N/A')}")

# Print SSE
print(f"\nSum of Squared Errors (SSE):  {fit.sse:.2f}")

"""## **Final Data** (cleaned data)"""

# Bulk_PA_original_data_copy to timestamp & datetime
Bulk_PA_final['proxy_transit_month'] = Bulk_PA_final['proxy_transit_month'].dt.to_timestamp()
Bulk_PA_final['proxy_transit_month'] = pd.to_datetime(Bulk_PA_final['proxy_transit_month'])

# Split Bulk_PA_original_data_copy for training & forecast - before & during El-Nino
Bulk_PA_final_train = Bulk_PA_final[(Bulk_PA_final['proxy_transit_month'] >= '2019-04-01') & (Bulk_PA_final['proxy_transit_month'] < '2023-06-01')]
Bulk_PA_final_observed = Bulk_PA_final[(Bulk_PA_final['proxy_transit_month'] >= '2023-06-01') & (Bulk_PA_final['proxy_transit_month'] < '2024-11-01')]

# Holt-Winters time-series decomposition

# Group by month to get the number of transits for the train dataset
monthly_transits_final = Bulk_PA_final_train.groupby('proxy_transit_month').size().rename("number_transits")
monthly_transits_final = monthly_transits_final.asfreq('MS')  # Ensure monthly frequency
# Group by month and count transits for the forecast dataset
monthly_counts_observed = Bulk_PA_final_observed.groupby('proxy_transit_month').size().reset_index(name='count')

# Plot the original series
monthly_transits_final.plot(title="Monthly Number of Transits", marker='o')
plt.ylabel("Transits")
plt.grid(True)
plt.show()

# Holt-Winters model (additive seasonality if there is clear monthly seasonality)
model = ExponentialSmoothing(
    monthly_transits_final,
    trend='add',
    damped_trend=True,  # Enable damping (to estimate phi)
    seasonal='add',
    seasonal_periods=12,
    initialization_method='estimated'
)
fit = model.fit(optimized=True)

# Forecast the next 18 months
forecast = fit.forecast(18)

# Plot the results
plt.figure(figsize=(10, 5))
plt.plot(monthly_transits_final, label='Observed')
plt.plot(fit.fittedvalues, label='Fitted')
plt.plot(forecast, label='Forecast', marker='o')
# Plot monthly_counts_observed
plt.plot(monthly_counts_observed['proxy_transit_month'], monthly_counts_observed['count'], label='Observed data', marker='x', color='red')
plt.title('Holt-Winters Forecast of Monthly Transits')
plt.legend()
plt.grid(True)
plt.show()

# Print smoothing parameters
params = fit.params
print("alpha:", params.get("smoothing_level"))
print("beta :", params.get("smoothing_trend"))
print("gamma:", params.get("smoothing_seasonal"))
print("phi  :", params.get("damping_trend"))

print("\nInitial values:")
print(f"  Initial level:               {params.get('initial_level', 'N/A')}")
print(f"  Initial trend:               {params.get('initial_trend', 'N/A')}")
print(f"  Seasonal initial values:     {params.get('initial_seasons', 'N/A')}")

# Print SSE
print(f"\nSum of Squared Errors (SSE):  {fit.sse:.2f}")

monthly_counts_observed

forecast

# Convert the forecast to a DataFrame
forecast_df = pd.DataFrame(forecast, columns=['forecast'])

# Add a date column from the index
forecast_df['date'] = forecast_df.index

# Reset the index
forecast_df.reset_index(drop=True, inplace=True)

# Switch the place of columns
forecast_df = forecast_df[['date', 'forecast']]

# Set the date column as the index
forecast_df.set_index

# Display the DataFrame
forecast_df

# Merge forecast_df and monthly_counts_observed
merged_df = pd.merge(forecast_df, monthly_counts_observed, left_on='date', right_on='proxy_transit_month', how='left')
# Drop proxy_transit_month column
merged_df.drop(columns=['proxy_transit_month'], inplace=True)
# Calculate the difference in the number of transits per month in int64 type
merged_df['difference'] = merged_df['forecast'].astype(int) - merged_df['count']
# Add a total count of missed transits
print('Number of deviated transits:', merged_df['difference'].sum())
# Display the new df
merged_df

# Read Bulk_PA_ML_1 from parquet
# Bulk_PA_ML_1 = pd.read_parquet('Bulk_PA_ML_1.parquet')
# Bulk_PA_ML_1

"""# **Alternative to Holt-Winters: Linear regression**

## **ML2: ML - Linear Regresion** region_pair & commodity details
"""

# Read Bulk_PA_ML_2 from parquet
Bulk_PA_ML_2 = pd.read_parquet('Bulk_PA_ML_2.parquet')
# See all columns
pd.set_option('display.max_columns', None)
Bulk_PA_ML_2

# Linear regression with Bulk_PA_ML_2
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Sort the data by date
Bulk_PA_ML_2 = Bulk_PA_ML_2.sort_values(by=['proxy_transit_month'])

# Create a TimeSeriesSplit object with n_splits=5
tscv = TimeSeriesSplit(n_splits=5)

# Prepare the data
X = Bulk_PA_ML_2[['ONI_lag_3', 'water_level_m_lag_3', 'VLSFO', 'Coal_Australian', 'Wheat_HRW', 'distance_94']]
y = Bulk_PA_ML_2['number_transits']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Iterate through the splits and train/evaluate the model
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model = LinearRegression()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error: {mse}")
    print(f"R-squared: {r2}")
    print("-" * 20)  # Separator between splits

"""## **ML4: statistical Linear Regression** - aggregated data"""

import statsmodels.formula.api as smf
from sklearn.model_selection import TimeSeriesSplit

# Read Bulk_PA_ML_4
Bulk_PA_ML_4 = pd.read_parquet('Bulk_PA_ML_4.parquet')

# Sort the data by date
Bulk_PA_ML_4 = Bulk_PA_ML_4.sort_values(by=['proxy_transit_month'])

# Create a TimeSeriesSplit object with n_splits=5
tscv = TimeSeriesSplit(n_splits=5)

# Convert proxy month to timestamp
Bulk_PA_ML_4['proxy_transit_month'] = Bulk_PA_ML_4['proxy_transit_month'].dt.to_timestamp()

# Create categorical features for dates & to object
Bulk_PA_ML_4['year'] = Bulk_PA_ML_4['proxy_transit_month'].dt.year.astype(str)
Bulk_PA_ML_4['month'] = Bulk_PA_ML_4['proxy_transit_month'].dt.month.astype(str)

# Define the formula for the model
formula = 'number_transits ~ year + month + VLSFO + Wheat_HRW'

# Iterate through the splits and train/evaluate the model
for train_index, test_index in tscv.split(Bulk_PA_ML_4):
    train_data = Bulk_PA_ML_4.iloc[train_index].copy()
    test_data = Bulk_PA_ML_4.iloc[test_index].copy()

    # Refit the model with the combined categories from train and test data
    # Get unique categories from both train and test data
    all_years = pd.concat([train_data['year'], test_data['year']]).unique()
    all_months = pd.concat([train_data['month'], test_data['month']]).unique()

    # Add these levels to the categorical dtype of the 'year' and 'month' column
    train_data['year'] = pd.Categorical(train_data['year'], categories=all_years)
    train_data['month'] = pd.Categorical(train_data['month'], categories=all_months)
    test_data['year'] = pd.Categorical(test_data['year'], categories=all_years)
    test_data['month'] = pd.Categorical(test_data['month'], categories=all_months)

    # Fit the model using smf.ols
    model = smf.ols(formula=formula, data=train_data).fit()

# Print model summary for each split
print(model.summary())

Bulk_PA_ML_4

"""## **ML5: statistical Linear Regression** - region pairs

### **Before El-Nino model**
"""

# Block 1 - initialization and correlation analysis before El-Nino
#Read Bulk_PA_ML_5
Bulk_PA_ML_5 = pd.read_parquet('Bulk_PA_ML_5.parquet')

# Sort the data by date
Bulk_PA_ML_5 = Bulk_PA_ML_5.sort_values(by=['proxy_transit_month']).reset_index(drop=True)

# Convert proxy_transit_month to period M
Bulk_PA_ML_5['proxy_transit_month'] = Bulk_PA_ML_5['proxy_transit_month'].dt.to_timestamp()

# Filter the data to exclude El-Nino data
Bulk_PA_ML_5_normal = Bulk_PA_ML_5[Bulk_PA_ML_5['proxy_transit_month'] < '2023-06-01'].copy()

# Correlation analysis between variables
# Select the columns for correlation analysis
columns_for_correlation = ['voy_intake_sum',	'number_transits',	'ONI',	'ONI_lag_1',	'ONI_lag_2',	'ONI_lag_3',	'water_level_m',	'water_level_m_lag_1',
                           'water_level_m_lag_2',	'water_level_m_lag_3',	'IFO380',	'Coal_Australian',	'Soybeans',	'Maize_Corn',	'Wheat_HRW',	'Phosphate_rock',	'Copper',
                           'voy_sea_duration_h',	'distance_94',
                           'vsl_dwt',	'fuel_consumption_mt', 'Australia-NE_America',	'E_Asia-NE_America',	'E_Asia-SE_America',
                           'Europe_N_Africa-NW_America',	'Europe_N_Africa-SW_America',	'NE_America-NW_America',	'NE_America-SW_America',	'NW_America-SE_America',
                           'SE_America-SW_America', 'dry_season',	'rain_season_1st_half',	'rain_season_2nd_half']
# Calculate the correlation matrix
correlation_matrix = Bulk_PA_ML_5_normal[columns_for_correlation].corr()

# Highlight the potential colinearrity

def highlight_cells(val):
  """
  Highlights cells with values greater than 0.5 in bold red.
  """
  if abs(val) > 0.5 and val != 1:
    color = 'red'
    weight = 'bold'
  else:
    color = 'black'
    weight = 'normal'
  return f'color: {color}; font-weight: {weight}'

# Apply the styling function to the correlation matrix
highlighted_correlation_matrix = correlation_matrix.style.applymap(highlight_cells)

# Display the styled correlation matrix
display(highlighted_correlation_matrix)

Bulk_PA_ML_5_normal.head()

# Block 2 - linear regression model
import statsmodels.formula.api as smf
from sklearn.model_selection import TimeSeriesSplit

# Create a TimeSeriesSplit object with n_splits=5
tscv = TimeSeriesSplit(n_splits=5)

# Define the formula for the model
formula = 'number_transits ~ region_pairs'
#formula = 'number_transits ~ region_pairs + IFO380	+ Coal_Australian + Soybeans + Maize_Corn +	Wheat_HRW +	Phosphate_rock +	Copper +	distance_94 + rain_season_1st_half +	rain_season_2nd_half'

# Convert 'region_pairs' to a categorical type using categories from the whole DataFrame
# This ensures all possible categories are known before splitting
Bulk_PA_ML_5_normal['region_pairs'] = pd.Categorical(Bulk_PA_ML_5_normal['region_pairs'],
                                                     categories=Bulk_PA_ML_5_normal['region_pairs'].unique())

# Iterate through the splits and train/evaluate the model
for train_index, test_index in tscv.split(Bulk_PA_ML_5_normal):
    train_data_ml_5_normal = Bulk_PA_ML_5_normal.iloc[train_index]
    test_data_ml_5_normal = Bulk_PA_ML_5_normal.iloc[test_index]

    # Fit the model using smf.ols
    model_ml_5_normal = smf.ols(formula=formula, data=train_data_ml_5_normal).fit()

    # Make predictions
    y_pred_ml_5_normal = model_ml_5_normal.predict(test_data_ml_5_normal)

# Store the last model's parameters
LR_5_normal_params = model_ml_5_normal.params

# Print model summary (includes parameters and significance)
print(model_ml_5_normal.summary())

# Linear relationship
# Create a scatter plot for region_pairs and number_transits
plt.figure(figsize=(10, 6))
sns.scatterplot(x='region_pairs', y='number_transits', data=Bulk_PA_ML_5_normal)

# Linear relationship - ANOVA test
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Fit a linear model with 'region_pairs' as a predictor
model = ols('number_transits ~ C(region_pairs)', data=Bulk_PA_ML_5_normal).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)

print(anova_table)

# Distribution of the errors from the model

# Calculate the residuals (this is correct as model_ml_5_normal.resid corresponds to the last trained fold)
Bulk_PA_ML_5_normal['residuals'] = model_ml_5_normal.resid

# Plot the distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(Bulk_PA_ML_5_normal['residuals'], bins=30, kde=True)
plt.title('Distribution of Model Residuals')
plt.xlabel('Residuals (Observed - Fitted)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()


# Main statistics of the distribution
print(Bulk_PA_ML_5_normal['residuals'].describe())

# Independence - VIF

# VIF of ml_model_5_normal
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Get the design matrix from the fitted model
X = model_ml_5_normal.model.exog

# Get the column names from the design matrix
column_names = model_ml_5_normal.model.exog_names

# Calculate VIF for each predictor
vif_data = pd.DataFrame()
vif_data["feature"] = column_names
vif_data["VIF"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]

# Display the VIF values
print("Variance Inflation Factors:")
print(vif_data)

# Test for homoscedasticity

# Calculate the residuals
Bulk_PA_ML_5_normal['residuals'] = model_ml_5_normal.resid

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=model_ml_5_normal.fittedvalues, y=Bulk_PA_ML_5_normal['residuals'])
plt.axhline(0, color='red', linestyle='--') # Add a horizontal line at 0
plt.title('Residuals vs Fitted Values for model_ml_5_normal')
plt.xlabel('Fitted Values (Predicted Number of Transits)')
plt.ylabel('Residuals (Observed - Fitted)')
plt.grid(True)
plt.show()

# Bulk_PA_ML_5_normal tp parquet
Bulk_PA_ML_5_normal.to_parquet('Bulk_PA_before_ElNino.parquet')

"""### **El-Nino model**"""

# Block 1 - initialization and correlation analysis before El-Nino

# Filter the data to only keep El-Nino data after 06-2023 and before 09-2024
Bulk_PA_ML_5_ElNino = Bulk_PA_ML_5[(Bulk_PA_ML_5['proxy_transit_month'] >= '2023-06-01') & (Bulk_PA_ML_5['proxy_transit_month'] < '2024-09-01')].copy()

# Correlation analysis between variables
# Select the columns for correlation analysis
columns_for_correlation = ['voy_intake_sum',	'number_transits',	'ONI',	'ONI_lag_1',	'ONI_lag_2',	'ONI_lag_3',	'water_level_m',	'water_level_m_lag_1',
                           'water_level_m_lag_2',	'water_level_m_lag_3',	'IFO380',	'Coal_Australian',	'Soybeans',	'Maize_Corn',	'Wheat_HRW',	'Phosphate_rock',	'Copper',
                           'voy_sea_duration_h',	'distance_94',
                           'vsl_dwt',	'fuel_consumption_mt', 'Australia-NE_America',	'E_Asia-NE_America',	'E_Asia-SE_America',
                           'Europe_N_Africa-NW_America',	'Europe_N_Africa-SW_America',	'NE_America-NW_America',	'NE_America-SW_America',	'NW_America-SE_America',
                           'SE_America-SW_America', 'dry_season',	'rain_season_1st_half',	'rain_season_2nd_half']
# Calculate the correlation matrix
correlation_matrix_ElNino = Bulk_PA_ML_5_ElNino[columns_for_correlation].corr()

# Apply the styling function to the correlation matrix
highlighted_correlation_matrixElNino = correlation_matrix_ElNino.style.applymap(highlight_cells)

# Display the styled correlation matrix
display(highlighted_correlation_matrixElNino)

# Block 1 - initialization and correlation analysis during and after El-Nino

# Create a TimeSeriesSplit object with n_splits=5
tscv = TimeSeriesSplit(n_splits=5)

# Define the formula for the model

formula = 'number_transits ~ region_pairs + ONI_lag_3'
# 'number_transits ~ region_pairs + water_level_m +  ONI_lag_3 + IFO380 + Maize_Corn + fuel_consumption_mt + rain_season_1st_half + rain_season_2nd_half'

# Convert 'region_pairs' to a categorical type using categories from the whole DataFrame
# This ensures all possible categories are known before splitting
Bulk_PA_ML_5_ElNino['region_pairs'] = pd.Categorical(Bulk_PA_ML_5_ElNino['region_pairs'],
                                                     categories=Bulk_PA_ML_5_ElNino['region_pairs'].unique())

# Iterate through the splits and train/evaluate the model
for train_index, test_index in tscv.split(Bulk_PA_ML_5_ElNino):
    train_data_ElNino = Bulk_PA_ML_5_ElNino.iloc[train_index]
    test_data_ElNino = Bulk_PA_ML_5_ElNino.iloc[test_index]

    # Fit the model using smf.ols
    model_ml_5_ElNino = smf.ols(formula=formula, data=train_data_ElNino).fit()

    # Make predictions
    y_pred = model_ml_5_ElNino.predict(test_data_ElNino)

# Store the last model's parameters
LR_5_ElNino_params = model_ml_5_ElNino.params

# Print model summary (includes parameters and significance)
print(model_ml_5_ElNino.summary())

# Linear relationship - ANOVA test

# Fit a linear model with 'region_pairs' as a predictor
model = ols('number_transits ~ C(region_pairs)', data=Bulk_PA_ML_5_ElNino).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)

print(anova_table)

# Linear relationship - ONI

# Scatter plot of ONI_lag3 and number_transits
plt.figure(figsize=(10, 6))
sns.scatterplot(x='ONI_lag_3', y='number_transits', data=Bulk_PA_ML_5_ElNino)

# Create a FacetGrid with 3 columns
g = sns.FacetGrid(Bulk_PA_ML_5_ElNino, col='region_pairs', col_wrap=3, height=4)

# Map the scatter plot onto the grid
g.map(sns.scatterplot, 'ONI_lag_3', 'number_transits')

# Add titles to each subplot
g.set_titles("ONI lag3 vs. Transits for {col_name}")

# Adjust layout
plt.tight_layout()
plt.show()

# Distribution of the errors from the model

# Calculate the residuals (this is correct as model_ml_5_ElNino.resid corresponds to the last trained fold)
Bulk_PA_ML_5_ElNino['residuals'] = model_ml_5_ElNino.resid

# Plot the distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(Bulk_PA_ML_5_ElNino['residuals'], bins=30, kde=True)
plt.title('Distribution of Model Residuals')
plt.xlabel('Residuals (Observed - Fitted)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()


# Main statistics of the distribution
print(Bulk_PA_ML_5_ElNino['residuals'].describe())

# Independence - VIF

# VIF of ml_model_5_ElNino
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Get the design matrix from the fitted model
X = model_ml_5_ElNino.model.exog

# Get the column names from the design matrix
column_names = model_ml_5_ElNino.model.exog_names

# Calculate VIF for each predictor
vif_data = pd.DataFrame()
vif_data["feature"] = column_names
vif_data["VIF"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]

# Display the VIF values
print("Variance Inflation Factors:")
print(vif_data)

# Test for homoscedasticity

# Calculate the residuals
Bulk_PA_ML_5_ElNino['residuals'] = model_ml_5_ElNino.resid

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=model_ml_5_ElNino.fittedvalues, y=Bulk_PA_ML_5_ElNino['residuals'])
plt.axhline(0, color='red', linestyle='--') # Add a horizontal line at 0
plt.title('Residuals vs Fitted Values for model_ml_5_ElNino')
plt.xlabel('Fitted Values (Predicted Number of Transits)')
plt.ylabel('Residuals (Observed - Fitted)')
plt.grid(True)
plt.show()

import statsmodels.formula.api as smf
import numpy as np
import pandas as pd

# Assuming Bulk_PA_ML_5_normal is your DataFrame

# Ensure 'number_transits' is positive before taking the logarithm
Bulk_PA_ML_5_normal = Bulk_PA_ML_5_normal[Bulk_PA_ML_5_normal['number_transits'] > 0].copy()

# Create the log of the dependent variable
Bulk_PA_ML_5_normal['log_number_transits'] = np.log(Bulk_PA_ML_5_normal['number_transits'])

# Define the formula with log(number_transits) as the dependent variable
# Include your desired independent variables
formula = 'log_number_transits ~ region_pairs'

# Fit the linear regression model
model = smf.ols(formula=formula, data=Bulk_PA_ML_5_normal).fit()

# Print the model summary
print(model.summary())

# Test for homoscedasticity

# Calculate the residuals
Bulk_PA_ML_5_ElNino['residuals'] = model.resid

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=model.fittedvalues, y=Bulk_PA_ML_5_normal['residuals'])
plt.axhline(0, color='red', linestyle='--') # Add a horizontal line at 0
plt.title('Residuals vs Fitted Values for model')
plt.xlabel('Fitted Values (Predicted Number of Transits)')
plt.ylabel('Residuals (Observed - Fitted)')
plt.grid(True)
plt.show()

"""### **Forecast 2023-2024 No ElNino**"""

# LR_5_normal_params to df - Linear regression parameters with data before El-Nino
LR_5_normal_params_df = pd.DataFrame(LR_5_normal_params).T
LR_5_normal_params_df
# Remove ']' and 'region_pairs[T.' in the column names
LR_5_normal_params_df.columns = LR_5_normal_params_df.columns.str.replace(']', '')
LR_5_normal_params_df.columns = LR_5_normal_params_df.columns.str.replace('region_pairs[T.', '')
LR_5_normal_params_df

# Forecast the transits for the El-Nino data
# Initialize the 'forecast' column with NaNs
Bulk_PA_ML_5_ElNino['forecast'] = np.nan

# Loop through each row of Bulk_PA_ML_5_ElNino using the index
for index in Bulk_PA_ML_5_ElNino.index:
    # Initialize the sum with the intercept value
    sum_value = LR_5_normal_params_df['Intercept'].iloc[0]

    # Loop through each column of LR_5_normal_params_df (excluding 'Intercept')
    for column_name in LR_5_normal_params_df.columns[1:]:
        # Get the corresponding column value from Bulk_PA_ML_5_ElNino
        # Use .loc with the index and column name
        column_value = Bulk_PA_ML_5_ElNino.loc[index, column_name]

        # Multiply the column value with the parameter value and add to the sum
        sum_value += LR_5_normal_params_df[column_name].iloc[0] * column_value

    # Assign the final sum to the 'forecast' column for the current row
    Bulk_PA_ML_5_ElNino.loc[index, 'forecast'] = sum_value

# Display the DataFrame with the forecast column
display(Bulk_PA_ML_5_ElNino.head())

# LR_5_ElNino_params to df - Linear regression parameters with data during El-Nino
LR_5_ElNino_params_df = pd.DataFrame(LR_5_ElNino_params).T
LR_5_ElNino_params_df
# Remove ']' and 'region_pairs[T.' in the column names
LR_5_ElNino_params_df.columns = LR_5_ElNino_params_df.columns.str.replace(']', '')
LR_5_ElNino_params_df.columns = LR_5_ElNino_params_df.columns.str.replace('region_pairs[T.', '')
LR_5_ElNino_params_df

# Forecast the transits for the El-Nino data
# Initialize the 'forecast' column with NaNs
Bulk_PA_ML_5_ElNino['forecastElNino'] = np.nan

# Loop through each row of Bulk_PA_ML_5_ElNino using the index
for index in Bulk_PA_ML_5_ElNino.index:
    # Initialize the sum with the intercept value
    sum_value = LR_5_ElNino_params_df['Intercept'].iloc[0]

    # Loop through each column of LR_5_normal_params_df (excluding 'Intercept')
    for column_name in LR_5_ElNino_params_df.columns[1:]:
        # Get the corresponding column value from Bulk_PA_ML_5_ElNino
        # Use .loc with the index and column name
        column_value = Bulk_PA_ML_5_ElNino.loc[index, column_name]

        # Multiply the column value with the parameter value and add to the sum
        sum_value += LR_5_ElNino_params_df[column_name].iloc[0] * column_value

    # Assign the final sum to the 'forecast' column for the current row
    Bulk_PA_ML_5_ElNino.loc[index, 'forecastElNino'] = sum_value

# Display the DataFrame with the forecast column
display(Bulk_PA_ML_5_ElNino.head())

# Create a rerouted voyage column - difference between forecast and actual number of transits
Bulk_PA_ML_5_ElNino['rerouted_voy'] = Bulk_PA_ML_5_ElNino['forecast'] - Bulk_PA_ML_5_ElNino['number_transits']
Bulk_PA_ML_5_ElNino['forecast_rerouted_voy'] = Bulk_PA_ML_5_ElNino['forecast'] - Bulk_PA_ML_5_ElNino['forecastElNino']
Bulk_PA_ML_5_ElNino.head()

#Create a column indexe which gives the value 1 to len(Bulk_PA_ML_5_normal)
Bulk_PA_ML_5_normal['indexe'] = range(1, len(Bulk_PA_ML_5_normal) + 1)
Bulk_PA_ML_5_normal.head()

# Graph of the rerouted voyages per month and region pair
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
sns.lineplot(x='proxy_transit_month', y='rerouted_voy', hue='region_pairs', data=Bulk_PA_ML_5_ElNino)
plt.title('Rerouted Voyages by Month and Region Pair')
plt.xlabel('Month')
plt.ylabel('Number of Rerouted Voyages')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Region Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')
# Add horizontal line at y=0
plt.axhline(0, color='black', linestyle='--')

plt.tight_layout()
plt.show()

"""If rerouted voyage is negative this means that the number of transits during El-Nino is higher than the number of transits before El-Nino. Therefore, this increase would already have been observed in the transits during El-Nino and no change has to be made. We change the number of rerouted voyages to 0 if the difference between the forecast and the data is negative."""

# Create a new DataFrame with the necessary columns
Bulk_PA_ML_5_ElNino_percentage = Bulk_PA_ML_5_ElNino[['proxy_transit_month', 'region_pairs', 'number_transits', 'forecast']].copy()

# Calculate the percentage of transits in the new DataFrame
Bulk_PA_ML_5_ElNino_percentage['transit_percentage'] = (Bulk_PA_ML_5_ElNino_percentage['number_transits'] / (Bulk_PA_ML_5_ElNino_percentage['forecast'])) * 100

# Graph of the transit voyages per month and region pair as percentage using the new DataFrame
plt.figure(figsize=(12, 8))
sns.lineplot(x='proxy_transit_month', y='transit_percentage', hue='region_pairs', data=Bulk_PA_ML_5_ElNino_percentage)
plt.title('Proportion of Total Voyages That Transited - Distribution by Region Pair')
plt.xlabel('Month')
plt.ylabel('Proportion of Transit Voyages (%)')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Region Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')
# Add a horizontal lines
plt.axhline(100, color='black', linestyle='--')
plt.axhline(0, color='black', linestyle='--')
plt.legend(title='Region Pairs', bbox_to_anchor=(1.05, 0.5), loc='center left')

# Set the y-axis tick locator to show steps of 10
plt.gca().yaxis.set_major_locator(mticker.MultipleLocator(20))
plt.grid(True)
plt.tight_layout()
plt.show()

# Graph of the rerouted voyages per month and region pair
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
sns.lineplot(x='proxy_transit_month', y='forecast_rerouted_voy', hue='region_pairs', data=Bulk_PA_ML_5_ElNino)
plt.title('Rerouted Voyages Forecasted by Month and Region Pair')
plt.xlabel('Proxy Transit Month')
plt.ylabel('Number of rerouted Voyages')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Region Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')
# Add horizontal line at y=0
plt.axhline(0, color='black', linestyle='--')

plt.tight_layout()
plt.show()

# Set 'rerouted_voy' to 0 if the difference is negative
Bulk_PA_ML_5_ElNino['rerouted_voy'] = Bulk_PA_ML_5_ElNino['rerouted_voy'].apply(lambda x: max(0, x))

# Graph of the rerouted voyages per month and region pair
plt.figure(figsize=(12, 6))  # Adjust figure size as needed
sns.lineplot(x='proxy_transit_month', y='rerouted_voy', hue='region_pairs', data=Bulk_PA_ML_5_ElNino)
plt.title('Rerouted Voyages by Month and Region Pair')
plt.xlabel('Month')
plt.ylabel('Number of Rerouted Voyages')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Region Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')
# Add horizontal line at y=0
plt.axhline(0, color='black', linestyle='--')

plt.tight_layout()
plt.show()

# Box plot of rerouted_voy
plt.figure(figsize=(10, 6))
sns.boxplot(x='rerouted_voy', data=Bulk_PA_ML_5_ElNino)
plt.xlabel('Number of rerouted voyages')
plt.show()

Bulk_PA_ML_5_ElNino[Bulk_PA_ML_5_ElNino['rerouted_voy'] <= 0]

# Graph with forecasted data of the two linear regression models and  observed data
monthly_data = Bulk_PA_ML_5_ElNino.groupby('proxy_transit_month').sum(numeric_only=True).reset_index()

# Plot the data
monthly_data.plot(x='proxy_transit_month', y=['number_transits', 'forecast', 'forecastElNino', 'rerouted_voy'],
                  kind='line', marker='o', figsize=(12, 6),
                  title='Comparison Between Linear Regression Models Forecast and Observed Data During El Nino')

plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.xlabel('Month')
plt.ylabel('Number of Transits')
plt.ylim(bottom=0)
plt.show()

"""### **Validity of the LR model**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Aggregate the number_transits and ONI_lag_3 by proxy_transit_month
monthly_data_elnino = Bulk_PA_ML_5_ElNino.groupby('proxy_transit_month').agg(
    number_transits=('number_transits', 'sum'),
    ONI_lag_3=('ONI_lag_3', 'mean')
).reset_index()

# Scatter plot with regression line
plt.figure(figsize=(10, 6))
sns.regplot(x='ONI_lag_3', y='number_transits', data=monthly_data_elnino)
plt.title('ONI vs. Number of Monthly Transits during El Nino')
plt.xlabel('ONI Value')
plt.ylabel('Number of Transits')
plt.grid(True)
plt.show()

# Scatter plot of region pair monthly transit
plt.figure(figsize=(10, 6))
sns.regplot(x='ONI_lag_3', y='number_transits', data=Bulk_PA_ML_5_ElNino)
plt.title('ONI vs. Number of Monthly Transits during El Nino per Region Pair')
plt.xlabel('ONI Value')
plt.ylabel('Number of Transits')
plt.grid(True)
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import numpy as np

# Assuming Bulk_PA_ML_5_ElNino is already loaded and available

# Select the variables
X = monthly_data_elnino[['ONI_lag_3']]
y = monthly_data_elnino['number_transits']

# Handle potential NaN values by dropping rows with NaNs in either column
data_for_r2 = monthly_data_elnino[['ONI_lag_3', 'number_transits']].dropna()
X_cleaned = data_for_r2[['ONI_lag_3']]
y_cleaned = data_for_r2['number_transits']


# Fit a simple linear regression model
model = LinearRegression()
model.fit(X_cleaned, y_cleaned)

# Predict on the cleaned data
y_pred = model.predict(X_cleaned)

# Calculate the R-squared score
r2 = r2_score(y_cleaned, y_pred)

print(f"R-squared between ONI_lag_3 and number_transits: {r2}")

# Distribution of monthly transits histogram during El Nino
plt.figure(figsize=(10, 6))
sns.histplot(data=Bulk_PA_ML_5_ElNino, x='number_transits', bins=20, edgecolor='black', kde=True)

plt.title('Distribution of Monthly Transits (During El Nino)')
plt.xlabel('Number of Transits')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Distribution of monthly transits histogram before El Nino

plt.figure(figsize=(10, 6))
sns.histplot(data=Bulk_PA_ML_5_normal, x='number_transits', bins=20, edgecolor='black', kde=True)

plt.title('Distribution of Monthly Transits (Before El Nino)')
plt.xlabel('Number of Transits')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Verification of the accuracy LR model - Apply the LR model to the whole dataset
# Initialize the 'forecast' column with NaNs
Bulk_PA_ML_5['forecast'] = np.nan

# Loop through each row of Bulk_PA_ML_5 using the index
for index in Bulk_PA_ML_5.index:
    # Initialize the sum with the intercept value
    sum_value = LR_5_normal_params_df['Intercept'].iloc[0]

    # Loop through each column of LR_5_normal_params_df (excluding 'Intercept')
    for column_name in LR_5_normal_params_df.columns[1:]:
        # Get the corresponding column value from Bulk_PA_ML_5
        # Use .loc with the index and column name
        column_value = Bulk_PA_ML_5.loc[index, column_name]

        # Multiply the column value with the parameter value and add to the sum
        sum_value += LR_5_normal_params_df[column_name].iloc[0] * column_value

    # Assign the final sum to the 'forecast' column for the current row
    Bulk_PA_ML_5.loc[index, 'forecast'] = sum_value

# Create a rerouted voyage column - difference between forecast and actual number of transits
Bulk_PA_ML_5['delta'] = Bulk_PA_ML_5['forecast'] - Bulk_PA_ML_5['number_transits']

# Display the DataFrame with the forecast column
display(Bulk_PA_ML_5.head())

# Group and sum data by month
monthly_data = Bulk_PA_ML_5.groupby('proxy_transit_month').sum(numeric_only=True).reset_index()

# Remove the last 2 months
monthly_data = monthly_data[:-2]

# Plot the data
monthly_data.plot(x='proxy_transit_month', y=['number_transits', 'forecast', 'delta'],
                  kind='line', marker='o', figsize=(12, 6),
                  title='Comparison Between Linear Regression Models Forecast and Observed Data')

plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.xlabel('Year')
plt.ylabel('Number of Transits')
plt.show()

# As the LR model looks valid, we are now ready to estimate the cost of rerouted vessels
# Bulk_PA_ML_5_ElNino to parquet
Bulk_PA_ML_5_ElNino.to_parquet('Bulk_rerouted.parquet')

"""## **ML6: statistical Linear Regression** - top commodity group

### **Before El-Nino model**
"""

#Read Bulk_PA_ML_6
Bulk_PA_ML_6 = pd.read_parquet('Bulk_PA_ML_6.parquet')

# Sort the data by date
Bulk_PA_ML_6 = Bulk_PA_ML_6.sort_values(by=['proxy_transit_month'])

# Convert proxy_transit_month to period M
Bulk_PA_ML_6['proxy_transit_month'] = Bulk_PA_ML_6['proxy_transit_month'].dt.to_timestamp()

# Filter the data to exclude El-Nino data
Bulk_PA_ML_6 = Bulk_PA_ML_6[Bulk_PA_ML_6['proxy_transit_month'] >= '2023-06-01']

# Create a TimeSeriesSplit object with n_splits=5
tscv = TimeSeriesSplit(n_splits=5)

# Define the formula for the model
formula = 'number_transits ~ commodity_group_top5 + ONI_lag_3 + rain_season_2nd_half + VLSFO + Wheat_HRW'
#formula = 'number_transits ~ year + month + region_pairs + ONI_lag_3 + water_level_m_lag_3 + IFO380 + Wheat_HRW'

# Iterate through the splits and train/evaluate the model
for train_index, test_index in tscv.split(Bulk_PA_ML_6):
    train_data = Bulk_PA_ML_6.iloc[train_index]
    test_data = Bulk_PA_ML_6.iloc[test_index]

    # Fit the model using smf.ols
    model_ml_6_ElNino = smf.ols(formula=formula, data=train_data).fit()

    # Make predictions
    y_pred = model_ml_6_ElNino.predict(test_data)

# Store the last model's parameters
LR_6_ElNino_params = model_ml_6_ElNino.params

# Print model summary (includes parameters and significance)
print(model_ml_6_ElNino.summary())

"""The Adj. R-squared is high, however, the coefficients are not logical. Therefore, ML 5 with region_pairs is better. Also, the Adj. R-squared of ML 5 is higher.

### **El_Nino model**
"""