# -*- coding: utf-8 -*-
"""Data Aggregation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t1bGNyaN1ix5vMoNy6uXWr2v8Q2G3aqR
"""

#Library import
import pyarrow as pa
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import statistics
import numpy as np
from datetime import datetime
import requests
from io import StringIO
import matplotlib.ticker as mticker

"""# **Addition of new variables** - ONI, water level, bunker price and commodity prices - from external sources

## **ONI - Oceanic Niño Index**
"""

# URL of the webpage containing the PSL file
url = 'https://psl.noaa.gov/data/correlation/oni.data'

# Fetch the content of the webpage
ONI_data = requests.get(url)
ONI_data.raise_for_status()  # Raise an exception for HTTP errors

# Use StringIO to read the PSL content into a pandas DataFrame
ONI = pd.read_csv(StringIO(ONI_data.text), delimiter='\t', header=None)

# Display the first few rows of the DataFrame
display(ONI)

# Adjust the table
ONI = ONI[1:76]

# Split data
ONI = ONI[0].str.split('  ', expand=True)
# Replace the header with months
ONI.columns = ['Year', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']

ONI

# Adjust the table to keep the relevant years (2018-2024) (Keep 2018 to introduce the lag feature)
ONI_19_24_pivot = ONI[68:]
ONI_19_24_pivot = ONI_19_24_pivot.reset_index(drop=True)
ONI_19_24_pivot

# Transpose the rows into a column
# Unpivot/melt the DataFrame
df_melted = ONI_19_24_pivot.melt(id_vars='Year', var_name='Month', value_name='ONI')

# Format 'Month' to two digits and create 'Year_Month'
df_melted['Month'] = df_melted['Month'].astype(str).str.zfill(2)
df_melted['Year_Month'] = df_melted['Year'].astype(str) + '-' + df_melted['Month']

# Create final DataFrame with only 'Year_Month' and 'Value'
ONI_19_24 = df_melted[['Year_Month', 'ONI']].copy()

# Sort the result
ONI_19_24 = ONI_19_24.sort_values('Year_Month').reset_index(drop=True)

# Display the result
display(ONI_19_24)

# Create a lag feature for ONI
for i in range(1, 5):  # Loop through lag values 1 to 4
    ONI_19_24[f'ONI_lag_{i}'] = ONI_19_24['ONI'].shift(i)
ONI_19_24

# Remove 2018 year (Remove the first 12 rows)
ONI_19_24 = ONI_19_24.iloc[12:]
ONI_19_24

# Download ONI_19_24 to parquet
#ONI_19_24.to_parquet('ONI_19_24.parquet')

"""### **Import Bulk_PA dataframe and merge ONI**"""

# Import the Bulk dataframe from parquet
Bulk_PA = pd.read_parquet('Bulk_PA_Clean.parquet')
#Bulk_PA = pd.read_parquet('Bulk_PA_original_data_no_duplicates.parquet')
# Display all the columns
pd.set_option('display.max_columns', None)

# Create a new column to estimate the transit month and convert to period M
Bulk_PA['proxy_transit_month'] = Bulk_PA['load_end_date'].dt.to_period('M')
# Convert ONI Year_Month dates to period M
if ONI_19_24['Year_Month'].dtype != 'period[M]':
  ONI_19_24['Year_Month'] = pd.to_datetime(ONI_19_24['Year_Month'].str.strip(), format='%Y-%m', errors='coerce')
  ONI_19_24['Year_Month'] = ONI_19_24['Year_Month'].dt.to_period('M')

# Add a new column for the ONI variable
Bulk_PA['ONI'] = np.nan

# Add ONI information to Bulk_PA
for index, row in Bulk_PA.iterrows():
    year_month = row['proxy_transit_month']
    matching_row = ONI_19_24[ONI_19_24['Year_Month'] == year_month]

    if not matching_row.empty:
        # Convert 'ONI' value to numeric before assigning
        try:
            ONI_value = pd.to_numeric(matching_row['ONI'].values[0])
            Bulk_PA.loc[index, 'ONI'] = ONI_value

            ONI_lag_1_value = pd.to_numeric(matching_row['ONI_lag_1'].values[0])
            Bulk_PA.loc[index, 'ONI_lag_1'] = ONI_lag_1_value

            ONI_lag_2_value = pd.to_numeric(matching_row['ONI_lag_2'].values[0])
            Bulk_PA.loc[index, 'ONI_lag_2'] = ONI_lag_2_value

            ONI_lag_3_value = pd.to_numeric(matching_row['ONI_lag_3'].values[0])
            Bulk_PA.loc[index, 'ONI_lag_3'] = ONI_lag_3_value
        except ValueError:
            print(f"Warning: Could not convert ONI value '{matching_row['ONI'].values[0]}' to numeric. Skipping.")

Bulk_PA

"""## **Water Level data** - Panama Canal Gatun Lake water level

Source: Panama Canal Authority (n.d.) https://evtms-rpts.pancanal.com/eng/h2o/index.html
"""

# Import and read the water level statistics from a csv file
WL_historic = pd.read_csv('Download_Gatun_Lake_Water_Level_History.csv')
WL_historic

# Convert to meters (as other variables are expreced in meters)
WL_historic['GATUN_LAKE_LEVEL(FEET)'] = WL_historic['GATUN_LAKE_LEVEL(FEET)'] * 0.3048

# Rename the columns for clarity
WL_historic = WL_historic.rename(columns={'GATUN_LAKE_LEVEL(FEET)': 'water_level_m'})
WL_historic = WL_historic.rename(columns={'DATE_LOG': 'Year_Month'})
# Convert to date time
WL_historic['Year_Month'] = pd.to_datetime(WL_historic['Year_Month'], format='%Y-%m-%d', errors='coerce')
WL_historic['water_level_m'] = WL_historic['water_level_m'].replace(0, np.nan)
WL_historic

# Create a new df with monthly mean only
WL_historic_mean = pd.DataFrame()
WL_historic_mean = WL_historic.groupby(WL_historic['Year_Month'].dt.to_period('M'))['water_level_m'].mean()
WL_historic_mean = WL_historic_mean.reset_index()

WL_historic_mean

import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter
from matplotlib.lines import Line2D

# Create a Graph of Monthly water level change per year for the whole dataset

# Copy the WL data frame for manipulation
WL_Graph = WL_historic.copy()

# Ensure datetime for Year Month
WL_Graph['Year_Month'] = pd.to_datetime(WL_Graph['Year_Month'])

# Extract the years
WL_Graph['Year'] = WL_Graph['Year_Month'].dt.year

# create a dummy date for the x-axis by combining the year 1900 with the month and day
# This avoids issues with invalid day/month combinations in the target year (1900)
WL_Graph['Month_Day'] = pd.to_datetime(WL_Graph['Year_Month'].dt.month.astype(str) + '-' + WL_Graph['Year_Month'].dt.day.astype(str) + '-1900', format='%m-%d-%Y', errors='coerce')


# pivot to wide form: index is Month_Day, columns are years
WL_Graph_pivot = WL_Graph.pivot(index='Month_Day', columns='Year', values='water_level_m')

# Create the Graph
fig, ax = plt.subplots(figsize=(12,8))

# Background lines (1965–2022)
hist_years = [y for y in WL_Graph_pivot.columns if 1965 <= y <= 2022]
for y in hist_years:
    ax.plot(WL_Graph_pivot.index, WL_Graph_pivot[y],
            color='lightgrey', linewidth=1)

# Highlight key drought years
highlights = {
    1998: {'color':'#FF7F0E','linewidth':2.5, 'label':'1998'},
    2016: {'color':'#F3E803','linewidth':2.5, 'label':'2016'},
    2023: {'color':'#e0c062','linewidth':2.5, 'label':'2023'},
}
for year, style in highlights.items():
    if year in WL_Graph_pivot.columns:
        ax.plot(WL_Graph_pivot.index, WL_Graph_pivot[year],
                color=style['color'],
                linewidth=style['linewidth'],
                label=str(year))

# Adjust the 2024 year
if 2024 in WL_Graph_pivot.columns:
    ax.plot(WL_Graph_pivot.index, WL_Graph_pivot[2024],
            color='#d82c20', linewidth=2.5,
            label='2024 (so far)')

# X-axis: month labels
ax.xaxis.set_major_locator(mdates.MonthLocator())
ax.xaxis.set_major_formatter(DateFormatter('%b'))
ax.set_xlim(pd.to_datetime('1900-01-01'), pd.to_datetime('1900-12-31'))

# X-axis label
ax.set_xlabel('Month', fontsize=12)

# Y-axis: meters
ax.set_ylabel('Daily Water level (m)', fontsize=12)

# Title & annotations
ax.set_title("Daily Gatun Lake Water Level Throughout the Years",
             fontsize=16, fontweight='bold', pad=20)

# Legend
legend_handles = [
    Line2D([0], [0], color='lightgrey', lw=1.5, label='1965–2022'),
    Line2D([0], [0], color='#FF7F0E', lw=2.5,   label='1998'),
    Line2D([0], [0], color='#F3E803', lw=2.5,   label='2016'),
    Line2D([0], [0], color='#e0c062', lw=2.5, label='2023'),
    Line2D([0], [0], color='#d82c20', lw=2.5, label='2024'),
]
ax.legend(handles=legend_handles, loc='lower right', frameon=False)

plt.tight_layout()
plt.show()

# Create a lag feature for water level
for i in range(1, 5):  # Loop through lag values 1 to 4
    WL_historic_mean[f'water_level_m_lag_{i}'] = WL_historic_mean['water_level_m'].shift(i)

# Keep relevant years (2019 - 2024)
WL_19_24 = WL_historic_mean[(WL_historic_mean['Year_Month'] >= '2019-01') & (WL_historic_mean['Year_Month'] < '2025-01')]
# Reset index
WL_19_24 = WL_19_24.reset_index(drop=True)
WL_19_24

# Add water level information to Bulk_PA
for index, row in Bulk_PA.iterrows():
    year_month = row['proxy_transit_month']
    matching_row = WL_19_24[WL_19_24['Year_Month'] == year_month]

    if not matching_row.empty:
        # Convert 'WL' value to numeric before assigning
        try:
            WL_value = pd.to_numeric(matching_row['water_level_m'].values[0])
            Bulk_PA.loc[index, 'water_level_m'] = WL_value

            WL_lag_1_value = pd.to_numeric(matching_row['water_level_m_lag_1'].values[0])
            Bulk_PA.loc[index, 'water_level_m_lag_1'] = WL_lag_1_value

            WL_lag_2_value = pd.to_numeric(matching_row['water_level_m_lag_2'].values[0])
            Bulk_PA.loc[index, 'water_level_m_lag_2'] = WL_lag_2_value

            WL_lag_3_value = pd.to_numeric(matching_row['water_level_m_lag_3'].values[0])
            Bulk_PA.loc[index, 'water_level_m_lag_3'] = WL_lag_3_value

        except ValueError:
            print(f"Warning: Could not convert WL value '{matching_row['water_level_m'].values[0]}' to numeric. Skipping.")

Bulk_PA

"""## **Bunker Price data** - IFO380

Data source: United States Department of Agriculture (2025) https://agtransport.usda.gov/Fuel/Daily-Bunker-Fuel-Prices/4v3x-mj86/about_data
"""

# Read Daily bunker fuel prices from excel
Daily_Bunker = pd.read_csv('Daily_Bunker_Fuel_Prices_20250402.csv')
Daily_Bunker

# Convert Day value to dt64
Daily_Bunker['Day'] = pd.to_datetime(Daily_Bunker['Day'], format='%m/%d/%Y')
Daily_Bunker

# Keep the relevant fuel type and years (2019 - 2024)
Daily_IFO380 = Daily_Bunker[['Day', 'Intermdiate Fuel Oil, 380cSt']].copy()
Daily_IFO380 = Daily_IFO380[(Daily_IFO380['Day'] >= '2019-01-01') & (Daily_IFO380['Day'] < '2025-01-01')]
Daily_IFO380

# Rename columns
Daily_IFO380 = Daily_IFO380.rename(columns={'Day': 'Year_Month'})
Daily_IFO380 = Daily_IFO380.rename(columns={'Intermdiate Fuel Oil, 380cSt': 'IFO380'})
Daily_IFO380

# Group by month and calculate the mean IFO380 values
IFO380_19_24 = Daily_IFO380.groupby(Daily_IFO380['Year_Month'].dt.to_period('M'))['IFO380'].mean().reset_index()
# Display the result
IFO380_19_24

# Add IFO380 information to Bulk_PA
for index, row in Bulk_PA.iterrows():
    year_month = row['proxy_transit_month']
    matching_row = IFO380_19_24[IFO380_19_24['Year_Month'] == year_month]

    if not matching_row.empty:
        # Convert 'IFO380' value to numeric before assigning
        try:
            IFO380_value = pd.to_numeric(matching_row['IFO380'].values[0])
            Bulk_PA.loc[index, 'IFO380'] = IFO380_value
        except ValueError:
            print(f"Warning: Could not convert IFO380_value '{matching_row['IFO380'].values[0]}' to numeric. Skipping.")

Bulk_PA

"""## **Commodity Prices**

### **Commodity price selection**

Overall view of the commodities distribution from Bulk dataframe
"""

# Distribution of the number of transits per commodity
# List commodities and their groups, then display the top 10
commodity_group_counts = Bulk_PA.groupby(['commodity_group', 'commodity'])['commodity'].count().reset_index(name='count')

# Sort by count in descending order, take the top 10, and reset the index
top_10_commodities = commodity_group_counts.sort_values('count', ascending=False).reset_index(drop=True)

# Calculate total commodity count in Bulk_PA
total_commodity_count = Bulk_PA['commodity'].count()

# Add a percentage column
top_10_commodities['percentage'] = (top_10_commodities['count'] / total_commodity_count * 100).map('{:.2f}%'.format)

# Calculate cumulative percentage
top_10_commodities['cumulative_percentage'] = top_10_commodities['count'].cumsum() / total_commodity_count * 100
top_10_commodities['cumulative_percentage'] = top_10_commodities['cumulative_percentage'].map('{:.2f}%'.format)


# Display the top 10 with percentage and cumulative percentage
display(top_10_commodities)

# Distribution of the number of transits per commodity group
# Group by commodity group and count
commodity_group_counts = Bulk_PA.groupby('commodity_group')['commodity_group'].count().reset_index(name='count')

# Sort by count and reset index
commodity_groups = commodity_group_counts.sort_values('count', ascending=False).reset_index(drop=True)

# Calculate total commodity group count
total_commodity_group_count = Bulk_PA['commodity_group'].count()

# Add percentage column
commodity_groups['percentage'] = (commodity_groups['count'] / total_commodity_group_count * 100).map('{:.2f}%'.format)

# Calculate and add cumulative percentage column
commodity_groups['cumulative_percentage'] = (commodity_groups['count'].cumsum() / total_commodity_group_count * 100).map('{:.2f}%'.format)

# Display the commodity groups with percentage and cumulative percentage in descending order
display(commodity_groups)

"""### **Import of commodity prices**

Import commodity prices to merge with our dataframe. Some commodities are missing from this dataset.

Source: World Bank (2025) https://www.worldbank.org/en/research/commodity-markets
"""

# Read commodity price data from xlsx file
Commodity_prices = pd.read_excel('CMO-Historical-Data-Monthly.xlsx', sheet_name='Monthly Prices Top 6')
Commodity_prices.head()

# Convert Year_Month from object to period M
Commodity_prices['Year_Month'] = pd.to_datetime(Commodity_prices['Year_Month'], format='%Y-%m', errors='coerce').dt.to_period('M')

# Create new columns in Bulk_PA
new_columns = ['Coal_Australian', 'Soybeans', 'Maize_Corn', 'Wheat_HRW', 'Phosphate_rock', 'Copper', 'Soybean_meal', 'Sugar', 'Urea', 'Potassium_chloride', 'Zinc', 'Wood_Pellets', 'Coking_Coal', 'HRC_Steel', 'Steel', 'Petroleum_Coke', 'Salt']
for column in new_columns:
    Bulk_PA[column] = np.nan

# Add comodity price information to Bulk_PA
for index, row in Bulk_PA.iterrows():
    year_month = row['proxy_transit_month']
    matching_row = Commodity_prices[Commodity_prices['Year_Month'] == year_month]

    if not matching_row.empty:
        for column in new_columns:  # Iterate through new_columns
            try:
                commodity_value = pd.to_numeric(matching_row[column].values[0])
                Bulk_PA.loc[index, column] = commodity_value  # Assign to the correct column
            except (ValueError, KeyError):  # Handle potential errors
                print(f"Warning: Could not convert or find value for '{column}' in Year_Month '{year_month}'. Skipping.")

Bulk_PA

"""## **Cargo Value**"""

# Step 1: Create the 'cargo_value' column
Bulk_PA['cargo_value'] = np.nan # Initialize with NaN

# Step 2: Define a dictionary to map commodities to prices
commodity_price_mapping = {
    'Corn': 'Maize_Corn',
    'Soybeans': 'Soybeans',
    'Steam Coal': 'Coal_Australian',
    'Salt': 'Salt',
    'Petroleum Coke': 'Petroleum_Coke',
    'Steels': 'Steel',
    'Wheat': 'Wheat_HRW',
    'Phosphate Rock': 'Phosphate_rock',
    'Sugar': 'Sugar',
    'Steel Slabs': 'HRC_Steel',
    'Coking Coal': 'Coking_Coal',
    'Urea': 'Urea',
    'Zinc Concentrates': 'Zinc',
    'Copper Cathodes': 'Copper',
    'Soybean Meal': 'Soybean_meal',
    'Muriate Of Potash': 'Potassium_chloride',
    'Wood Pellets': 'Wood_Pellets'
}

# Step 3: Calculate 'cargo_value' based on commodity and price
for index, row in Bulk_PA.iterrows():
    commodity = row['commodity']
    if commodity in commodity_price_mapping:
        price_column = commodity_price_mapping[commodity]
        if price_column in Bulk_PA.columns and not pd.isnull(row[price_column]):  # Check if the price column exists and value is not NaN
            Bulk_PA.loc[index, 'cargo_value'] = row[price_column] * row['voy_intake_mt']
Bulk_PA

# Average of cargo value
Bulk_PA['cargo_value'].mean()

# Calculate the mean of the 'cargo_value' column, excluding NaN values
mean_cargo_value = Bulk_PA['cargo_value'].mean()

# Fill the NaN values in 'cargo_value' with the calculated mean
Bulk_PA['cargo_value'] = Bulk_PA['cargo_value'].fillna(mean_cargo_value)

# Display the DataFrame to see the changes (optional)
display(Bulk_PA.head())

# Check if there are any remaining NaN values in the column (optional)
print("\nNaN count after filling:")
print(Bulk_PA['cargo_value'].isnull().sum())

# Cargo value analytics
# Box plot
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
sns.boxplot(y=Bulk_PA['cargo_value'])
plt.title('Box Plot of Cargo Value')
plt.ylabel('Cargo Value')
plt.show()

Bulk_PA

# Cargo value box plot per region pair
plt.figure(figsize=(15, 8))  # Adjust figure size as needed
sns.boxplot(x='region_pairs', y='cargo_value', data=Bulk_PA)
plt.title('Box Plot of Cargo Value per Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Cargo Value')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent labels from overlapping
plt.show()

Bulk_PA['cargo_value'].describe()

# Cargo value in descending order
Bulk_PA.sort_values(by=['cargo_value'], ascending=False).head(20)

"""## **Canal fee**

In order to calculate the canal passage fee, we must estimate the Vessel length over all (LOA) and the beam length.
We use S. C Misra (2016) p 115 for Length Between Perpendiculars (LBP) estimation. To convert LBP to LOA, a multiplier of 1.04 is used from Tsujimoto et al (2019). FInally, for beam estimation, a function containing LOA is used from Misra (2016) p.116. An alternative version could be seen in https://www.yumpu.com/en/document/view/11297041/empirical-ship-formula
Canal fee data was retreived directly from the Panam Canal Authority website. https://pancanal.com/wp-content/uploads/2024/09/PP-Lista-tarifas-consolidado-final-ingles.pdf; Additional documentation was used to clarify some of the passage fees:
*   Vessel type and unit of surcharge calculation: https://pancanal.com/wp-content/uploads/2022/09/Approved-Tolls-FY23_24_25-for-website_rev-6Sep2022-1.pdf; Note: For dry bulk carriers, DWT is the unit of estimating the calculation of tolls.
*   Line handlers and locomotives: https://pancanal.com/wp-content/uploads/2021/08/ENGLISH-FAQ-OMS-2024.pdf
*   Fresh water surcharge: https://pancanal.com/wp-content/uploads/2021/08/Notes-on-tolls-maritime-tariffs-April-2025.pdf; Note, for practical purposes, the variable part is estimated on the monthly water level average on the transit month rather than the previous day of passage. This change is not significant as this cost varies between 1 and 10%. As the water level change is gradual over a month, the average of a month is considered as a good enogh proxy.
Finally, to verify the validity of the calculation, the toll calculator from Wilhelmsen was used: https://www.wilhelmsen.com/tollcalculators/panama-toll-calculator/.
Note, the canal toll fee includes only mandatory elements and do not include optional services.
"""

# Adjust the voyage draft of the vessels if it is larger than the maximum draft of the vessel
# so to reduce human input and AIS errors and better represent the toll fees
for index, row in Bulk_PA.iterrows():
    if row['load_draft'] > row['vsl_max_draft']:
        Bulk_PA.loc[index, 'load_draft'] = row['vsl_max_draft']

1 # Add vsl displacement
#Bulk_PA['vsl_displacement'] = np.nan

# Add vsl length
Bulk_PA['vsl_LOA'] = np.nan
Bulk_PA['vsl_LOA'] = 1.04*6.667*Bulk_PA['vsl_dwt']**0.308

# Add vsl beam
Bulk_PA['vsl_beam'] = np.nan
Bulk_PA['vsl_beam'] = 0.164*(6.667*Bulk_PA['vsl_dwt']**0.308)+0.090

# Round vessel dimensions to make it more realistic. No vessle will be 10 cm larger than the Panamax locks, \
# as it will be unreasonable from an economic point of view as the toll is much larger
# Round vsl_beam to 32.3 if it's between 32 and 33.5
Bulk_PA.loc[(Bulk_PA['vsl_beam'] >= 32) & (Bulk_PA['vsl_beam'] <= 33.5), 'vsl_beam'] = 32.3

# Round vsl_LOA to 294 if it's between 293 and 295
Bulk_PA.loc[(Bulk_PA['vsl_LOA'] >= 293) & (Bulk_PA['vsl_LOA'] <= 295), 'vsl_LOA'] = 294

# Estimate the ship size according to PCA definition
# Add ship size
Bulk_PA['vsl_ship_size'] = np.nan
# Define a function to categorize vessels
def categorize_vessel(row):
    if row['vsl_beam'] > 32.61 or row['vsl_LOA'] > 294.44 or row['load_draft'] >= 12.12:
        return 'Neopanamax'
    elif row['vsl_beam'] < 27.74:
        return 'Regular'
    elif row['vsl_beam'] >= 27.74:
        return 'Super'
    else:
        return None  # Or handle cases that don't match any category

# Apply the function to create the 'vsl_ship_size' column
Bulk_PA['vsl_ship_size'] = Bulk_PA.apply(categorize_vessel, axis=1)

# Canal fee estimation
import math
# Create a Canal fee column
Bulk_PA['canal_fee'] = np.nan

def calculate_panama_canal_fee(vessel_type: str, dwt: float, loa: float, beam: float, draft: float, water_level_m: float, laden: bool = True) -> float:
    """
    Estimate Panama Canal transit cost based on 2024 tariffs, with dynamic fresh water surcharge and updated ancillary fees.
    Returns the total estimated cost in USD.
    """
    # Fixed toll and DWT tariffs
    fixed_fee = {
        'Regular': 60000,
        'Super': 100000,
        'Neopanamax': 300000
    }

    capacity_tariff = {
        'Regular': 1.25,
        'Super': 1.15,
        'Neopanamax': 0.50
    }

    if vessel_type not in fixed_fee:
        raise ValueError(f"Unknown vessel type: {vessel_type}")

    # Tug assistance based on size
    if vessel_type == 'Neopanamax':
        tug_assistance = 30000
    elif 213.36 <= loa <= 294.44 or beam <= 32.61:
        tug_assistance = 11600
    elif loa < 213.36 and 24.38 <= beam <= 27.73:
        tug_assistance = 7000
    elif loa < 173.74 and beam < 24.38:
        tug_assistance = 5000
    else:
        raise ValueError(f"Unmatched tug assistance rule for vessel_type={vessel_type}, LOA={loa}, Beam={beam}")

    # Linehandlers cost
    if vessel_type == 'Neopanamax':
        num_linehandlers = 12
        linehandler_cost_per = 325
    elif loa > 259 and draft >= 10.97:
        num_linehandlers = 24
        linehandler_cost_per = 270
    elif loa > 182.9 or dwt > 30000:
        num_linehandlers = 19
        linehandler_cost_per = 270
    elif 152.4 < loa <= 182.8 or 22000 < dwt <= 30000:
        num_linehandlers = 14
        linehandler_cost_per = 270
    elif 38.1  < loa <= 173.7 or 12000 < dwt <= 22000:
        num_linehandlers = 14
        linehandler_cost_per = 270
    else:
        raise ValueError(f"Unmatched linehandler rule for vessel_type={vessel_type}, LOA={loa}, DWT={dwt}, Draft={draft}")

    linehandlers_total = num_linehandlers * linehandler_cost_per

    # Locomotive wire cost
    if vessel_type == 'Neopanamax':
        locomotive_cost = 0
    elif num_linehandlers == 11:
        locomotive_cost = 4 * 500
    elif num_linehandlers in [12, 14]:
        locomotive_cost = 8 * 500
    elif num_linehandlers == 19:
        locomotive_cost = 12 * 500
    elif num_linehandlers == 24:
        locomotive_cost = 16 * 500
    else:
        raise ValueError(f"Unmatched locomotive wire rule for linehandlers={num_linehandlers}")

    # Base toll
    fixed = fixed_fee[vessel_type]
    capacity = capacity_tariff[vessel_type] * dwt
    base_toll = fixed + capacity

    # Fresh water surcharge variable component (as % of base toll) if transit month starts from 01-2024
    if row['proxy_transit_month'] <= pd.Period('2024-01', freq='M'):
      freshwater_variable_pct = 0.1 / (1 + math.exp(0.6 * ((water_level_m * 3.280839895) - 79)))
      freshwater_variable = freshwater_variable_pct * base_toll
    else:
      freshwater_variable = 0

    # Other mandatory costs (tug, linehandlers, locomotives already included)
    ancillary_fees = {
        "Canal Inspection": 670,
        "Security Surcharge": 1250,
        "Fresh Water Surcharge": 10000 + freshwater_variable
    }

    ancillary_total = tug_assistance + linehandlers_total + locomotive_cost + sum(ancillary_fees.values())
    grand_total = base_toll + ancillary_total
    return grand_total

# Apply to DataFrame
Bulk_PA['canal_fee'] = Bulk_PA.apply(
    lambda row: calculate_panama_canal_fee(
        row['vsl_ship_size'], row['vsl_dwt'], row['vsl_LOA'], row['vsl_beam'], row['load_draft'], row['water_level_m']
    ), axis=1
)

# Graph of voyage intake distribution
# Group by proxy_transit_month and calculate the mean of voy_intake_mt
monthly_average_intake = Bulk_PA.groupby('proxy_transit_month')['voy_intake_mt'].mean()

# Convert the Period objects to datetime objects for plotting
monthly_average_intake.index = monthly_average_intake.index.to_timestamp()

# Create the plot
plt.figure(figsize=(12, 6))
plt.plot(monthly_average_intake.index, monthly_average_intake.values, marker='o')
plt.xlabel('Month')
plt.ylabel('Voyage Intake (mt)')
plt.title('Monthly Average Voyage Intake (2019-2024)')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

Bulk_PA

# Create a copy of the original DataFrame
Bulk_PA_ratio = Bulk_PA.copy()

# Calculate the load ratio and assign it to a new column
Bulk_PA_ratio['load_ratio'] = Bulk_PA_ratio['voy_intake_mt'] / Bulk_PA_ratio['vsl_dwt']

# Display the new DataFrame with the load ratio column
Bulk_PA_ratio.describe()

Bulk_PA_ratio.sort_values(by=['load_ratio'], ascending=True)

none_count = Bulk_PA['commodity'].isnull().sum()
print(f"Number of 'None' values in 'commodity': {none_count}")

# Group by 'proxy_transit_month' and count null values in 'commodity'
none_counts_per_month = Bulk_PA.groupby('proxy_transit_month')['commodity'].apply(lambda x: x.isnull().sum()).reset_index(name='none_count')

# Display the result
#show all rows
pd.set_option('display.max_rows', None)
print(none_counts_per_month)

# Stop showing all rows
pd.reset_option('display.max_rows')
Bulk_PA.describe()

# List the frequency of each vessel size
Bulk_PA['vsl_ship_size'].value_counts()

# DWT distribution
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
sns.histplot(Bulk_PA['canal_fee'], bins=20, kde=True)
plt.title('Canal fee')
plt.xlabel('USD')
plt.ylabel('Frequency')
plt.show()

"""This dataset will help us to structure the dataset for our linear regression models"""

# Extract the final dataset for machine learning
Bulk_PA.to_parquet('Bulk_PA_final.parquet')

# Scatter plot the number of monthly transits & highlight the El Nino period
monthly_counts = Bulk_PA.groupby('proxy_transit_month').size().reset_index(name='count')

# Filter data to be between 2019 and 2025
# Explicitly create a copy after filtering
monthly_counts_filtered = monthly_counts[(monthly_counts['proxy_transit_month'] >= '2019-01') & (monthly_counts['proxy_transit_month'] <= '2024-10')].copy()

# Convert the Period objects to datetime objects for plotting
monthly_counts_filtered['proxy_transit_month'] = monthly_counts_filtered['proxy_transit_month'].dt.to_timestamp()

# Create plot
plt.figure(figsize=(12, 6))
plt.plot(monthly_counts_filtered['proxy_transit_month'], monthly_counts_filtered['count'], marker='o')
plt.xlabel('Year')
plt.ylabel('Number of Transits')
plt.title('Monthly Transits (2019-2024)')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.ylim(bottom=0)

# Highlight the El Niño period (June 2023 to August 2024)
el_nino_start = pd.to_datetime('2023-06')
el_nino_end = pd.to_datetime('2024-08')
plt.axvspan(el_nino_start, el_nino_end, color='red', alpha=0.2, label='El Niño Period')

plt.legend()
plt.show()

"""# **Dataset preparation**

## **Commodity group categorization**
"""

# Display the commodity groups with percentage and cumulative percentage in descending order
# Same table as shown before
display(commodity_groups)

"""As the top five commodity groups cover over 88% of the transits, they are sufficient for our further analysis. It allows to reduce dimensionality and all categories now contain sufficient number of observations for multiple linear regression."""

# Create a new variable to categorise the commodities in 6 groups
top5_groups = ['Grain', 'Bulk', 'Steels', 'Fertilizers', 'Coal']

Bulk_PA['commodity_group_top5'] = Bulk_PA['commodity_group'].apply(lambda x: x if x in top5_groups else 'Others')
Bulk_PA.head()

# Commodity groups distribution per region pair

# Assuming 'Bulk_PA' is your original DataFrame
new_df = Bulk_PA[['proxy_transit_month', 'commodity_group_top5', 'commodity_group', 'commodity', 'region_pairs']].copy()

# Group by 'commodity_group_top5' and 'region_pairs'
grouped_df = new_df.groupby(['commodity_group_top5', 'region_pairs'])

# Calculate size, unstack, and reset index to get the desired output
result_df = grouped_df.size().unstack(fill_value=0).reset_index()

# Display the resulting DataFrame
result_df

import math
from matplotlib.lines import Line2D
# Create pie charts for the distribution of commodity groups per region pair before and during El Nino
# Filter data before June 2023
before_el_nino = Bulk_PA[Bulk_PA['proxy_transit_month'] < pd.Period('2023-06', freq='M')].copy()

# Filter data during El Niño (June 2023 to August 2024 as per your previous code)
during_el_nino = Bulk_PA[(Bulk_PA['proxy_transit_month'] >= pd.Period('2023-06', freq='M')) & (Bulk_PA['proxy_transit_month'] <= pd.Period('2024-09', freq='M'))].copy()


# Get unique region pairs present in either period
all_region_pairs = pd.concat([before_el_nino['region_pairs'], during_el_nino['region_pairs']]).unique()
n_regions = len(all_region_pairs)

# Define colors for each commodity group
commodity_colors = {
    'Grain': '#F5DEB3',
    'Bulk': '#FFA07A',
    'Steels': '#C0C0C0',
    'Fertilizers': '#87CEFA',
    'Coal': '#708090',
    'Others': '#F0FFFF'
}

# Set up the figure and axes for a 2xN grid (N rows, 2 columns for before/during)
cols = 2
rows = math.ceil(n_regions)

fig, axes = plt.subplots(rows, cols, figsize=(10, 5 * rows)) # Adjusted figsize

axes = axes.flatten() # Flatten the 2D array of axes for easy iteration

# Iterate through each region pair
for i, region_pair in enumerate(all_region_pairs):
    # Before El Niño pie chart
    ax_before = axes[i * cols]
    data_before = before_el_nino[before_el_nino['region_pairs'] == region_pair]
    dist_before = data_before['commodity_group_top5'].value_counts()
    colors_before = [commodity_colors.get(g, '#CCCCCC') for g in dist_before.index]

    if not dist_before.empty: # Check if there's data for this period
      ax_before.pie(
          dist_before,
          labels=dist_before.index,
          autopct='%1.1f%%',
          startangle=90,
          colors=colors_before
      )
    ax_before.set_title(f'{region_pair} (Before El Niño)')

    # During El Niño pie chart
    ax_during = axes[i * cols + 1]
    data_during = during_el_nino[during_el_nino['region_pairs'] == region_pair]
    dist_during = data_during['commodity_group_top5'].value_counts()
    colors_during = [commodity_colors.get(g, '#CCCCCC') for g in dist_during.index]

    if not dist_during.empty: # Check if there's data for this period
      ax_during.pie(
          dist_during,
          labels=dist_during.index,
          autopct='%1.1f%%',
          startangle=90,
          colors=colors_during
      )
    ax_during.set_title(f'{region_pair} (During El Niño)')


# Hide any unused subplots if the number of regions is odd
if n_regions * cols > len(axes):
    for j in range(n_regions * cols, len(axes)):
        fig.delaxes(axes[j])

# Overall title and layout
fig.suptitle('Commodity Distribution by Region Pair (Before vs. During El Niño)', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for the super title

plt.show()

# Commodity analysis - analyse in further details which segments were the most impacted and between which regions.
# Careful the timeframe is different: 53 months VS 15 months --> Should be modify to consider the monthly level.

def create_region_table(region_pair):
    # Filter data for the region pair
    region_data = Bulk_PA[Bulk_PA['region_pairs'] == region_pair]

    # Calculate transits before June 2023 at the monthly level
    before_june = region_data[region_data['proxy_transit_month'] < pd.Period('2023-06', freq='M')]
    before_counts1 = before_june['commodity_group_top5'].value_counts()
    before_counts = before_counts1/53
    before_total = before_counts.sum()
    before_percentages = (before_counts / before_total * 100).round(2)

    # Calculate transits after and including June 2023 at the monthly level
    after_june = region_data[region_data['proxy_transit_month'] >= pd.Period('2023-06', freq='M')]
    after_counts1 = after_june['commodity_group_top5'].value_counts()
    after_counts = after_counts1/19
    after_total = after_counts.sum()
    after_percentages = (after_counts / after_total * 100).round(2)

    # Get all unique commodity groups from both periods
    all_commodity_groups = before_counts.index.union(after_counts.index)

    # Reindex both series with all commodity groups, filling missing values with 0
    before_counts = before_counts.reindex(all_commodity_groups, fill_value=0)
    before_percentages = before_percentages.reindex(all_commodity_groups, fill_value=0)
    after_counts = after_counts.reindex(all_commodity_groups, fill_value=0)
    after_percentages = after_percentages.reindex(all_commodity_groups, fill_value=0)

    # Create the table
    table_data = {
        'Commodity Group': all_commodity_groups,
        'Before June Count': before_counts.values,
        'Before June %': before_percentages.values,
        'After June Count': after_counts.values,
        'After June %': after_percentages.values,
        '% point Change': ((after_percentages - before_percentages)).round(2)
    }
    table = pd.DataFrame(table_data)
    table.index = table['Commodity Group']
    table = table.drop(columns=['Commodity Group'])
    table = table.fillna(0)
    return table

# Get unique region pairs
region_pairs = Bulk_PA['region_pairs'].unique()

# Create and display tables for each region pair
for region_pair in region_pairs:
    region_table = create_region_table(region_pair)
    print(f"Table for {region_pair}:\n")
    display(region_table)
    print("\n")

"""## **Structure of the dataframe for LR**

### **Model 1** Monthly count by region pair by commodity group
"""

# Group & aggregate data
Bulk_PA_ML_1 = (
    Bulk_PA
    .groupby(['proxy_transit_month', 'region_pairs', 'commodity_group_top5'], as_index=False)
    .agg(voy_intake_sum=('voy_intake_mt','sum'),
         number_transits=('load_start_date','count'),  # load_start_date is really precise and therefore
    #it is very unlikely that two vessels have the same load date considering the variable has minutes and seconds
        ONI=('ONI','mean'), ONI_lag_3=('ONI_lag_3','mean'), water_level_m=('water_level_m','mean'),
         water_level_m_lag_3=('water_level_m_lag_3','mean'), IFO380=('IFO380','mean'),
         Coal_Australian=('Coal_Australian','mean'), Soybeans=('Soybeans','mean'), Maize_Corn=('Maize_Corn','mean'),
         Wheat_HRW=('Wheat_HRW','mean'), Phosphate_rock=('Phosphate_rock','mean'),
         Copper=('Copper','mean'), voy_sea_duration_h=('voy_sea_duration_h','mean'),
         distance_94=('distance_94','mean')
         )

)

Bulk_PA_ML_1

# Export Bulk_PA_ML_1 to parquet
#Bulk_PA_ML_1.to_parquet('Bulk_PA_ML_1.parquet')

"""### **Model 2** one-hot encoding per region pair per commodity group"""

# Convert categorical variables to binary

# Create a copy of Bulk_PA_ML_1
Bulk_PA_ML_2 = Bulk_PA_ML_1.copy()

# Create binary variables for region_pairs
region_pairs_dummies = pd.get_dummies(Bulk_PA_ML_2['region_pairs'])
region_pairs_dummies = region_pairs_dummies.astype(int)  # Convert True/False to 1/0
Bulk_PA_ML_2 = pd.concat([Bulk_PA_ML_2, region_pairs_dummies], axis=1)


# Create binary variables for commodity_group_top5
commodity_group_dummies = pd.get_dummies(Bulk_PA_ML_2['commodity_group_top5'])
commodity_group_dummies = commodity_group_dummies.astype(int)  # Convert True/False to 1/0
Bulk_PA_ML_2 = pd.concat([Bulk_PA_ML_2, commodity_group_dummies], axis=1)

# Drop original columns
Bulk_PA_ML_2 = Bulk_PA_ML_2.drop(['region_pairs', 'commodity_group_top5'], axis=1)
Bulk_PA_ML_2

# Bulk_PA_ML_2 to parquet
#Bulk_PA_ML_2.to_parquet('Bulk_PA_ML_2.parquet')

Bulk_PA_ML_2

"""### **Model 3** Montlhly transits per region pair per commodity group filtered in alphabetic (descending) order"""

# Reorder columns
Bulk_PA_ML_3 = Bulk_PA_ML_1[['region_pairs', 'commodity_group_top5'] +
                           [col for col in Bulk_PA_ML_1.columns
                            if col not in ('region_pairs', 'commodity_group_top5')]]
# Sort the DataFrame
Bulk_PA_ML_3 = Bulk_PA_ML_3.sort_values(
    by=['region_pairs', 'commodity_group_top5', 'proxy_transit_month'],
    ascending=[True, True, True]
)

# Reset index
Bulk_PA_ML_3 = Bulk_PA_ML_3.reset_index(drop=True)
display(Bulk_PA_ML_3)

# Bulk_PA_ML_3 to parquet
#Bulk_PA_ML_3.to_parquet('Bulk_PA_ML_3.parquet')

"""### **Model 4** Aggregated monthly transits"""

# Group & aggregate data
Bulk_PA_ML_4 = (
    Bulk_PA
    .groupby(['proxy_transit_month'], as_index=False)
    .agg(voy_intake_sum=('voy_intake_mt','sum'),
         number_transits=('load_start_date','count'),  # load_start_date is really precise and therefore
    #it is very unlikely that two vessels have the same load date considering the variable has minutes and seconds
        ONI=('ONI','mean'), ONI_lag_3=('ONI_lag_3','mean'), water_level_m=('water_level_m','mean'),
         water_level_m_lag_3=('water_level_m_lag_3','mean'), IFO380=('IFO380','mean'),
         Coal_Australian=('Coal_Australian','mean'), Soybeans=('Soybeans','mean'), Maize_Corn=('Maize_Corn','mean'),
         Wheat_HRW=('Wheat_HRW','mean'), Phosphate_rock=('Phosphate_rock','mean'),
         Copper=('Copper','mean'), voy_sea_duration_h=('voy_sea_duration_h','mean'),
         distance_94=('distance_94','mean')
         )

)

Bulk_PA_ML_4

# Bulk_PA_ML_4 to parquet
#Bulk_PA_ML_4.to_parquet('Bulk_PA_ML_4.parquet')

"""### **Model 5** Monthly transits per region pair"""

# Group & aggregate data to fit the machine learning need
# First dataframe structure
Bulk_PA_ML_5 = (
    Bulk_PA
    .groupby(['proxy_transit_month', 'region_pairs'], as_index=False)
    .agg(voy_intake_sum=('voy_intake_mt','sum'),
         number_transits=('load_start_date','count'),  # load_start_date is really precise (as it has minutes and seconds) \
    # and therefore it is very unlikely that two vessels have the same load date
        ONI=('ONI','mean'), ONI_lag_1=('ONI_lag_1','mean'), ONI_lag_2=('ONI_lag_2','mean'), ONI_lag_3=('ONI_lag_3','mean'),
         water_level_m=('water_level_m','mean'), water_level_m_lag_1=('water_level_m_lag_1','mean'), water_level_m_lag_2=('water_level_m_lag_2','mean'),
         water_level_m_lag_3=('water_level_m_lag_3','mean'), IFO380=('IFO380','mean'),
         Coal_Australian=('Coal_Australian','mean'), Soybeans=('Soybeans','mean'), Maize_Corn=('Maize_Corn','mean'),
         Wheat_HRW=('Wheat_HRW','mean'), Phosphate_rock=('Phosphate_rock','mean'),
         Copper=('Copper','mean'), Soybean_meal=('Soybean_meal','mean'), Sugar=('Sugar','mean'), Urea=('Urea','mean'),
         Potassium_chloride=('Potassium_chloride','mean'), Zinc=('Zinc','mean'), Wood_Pellets=('Wood_Pellets','mean'),
         Coking_Coal=('Coking_Coal','mean'), HRC_Steel=('HRC_Steel','mean'), Steel=('Steel','mean'),
         Petroleum_Coke=('Petroleum_Coke','mean'), Salt=('Salt','mean'), voy_sea_duration_h=('voy_sea_duration_h','mean'),
         distance_94=('distance_94','median'), shortest_distance_nm=('shortest_distance_nm','mean'), shortest_distance_day=('shortest_distance_day','mean'),
         second_shortest_distance_nm=('second_shortest_distance_nm','mean'),  second_shortest_distance_day=('second_shortest_distance_day','mean'),
         rerouting_nm=('rerouting_nm','mean'), rerouting_day=('rerouting_day','mean'),
         vsl_dwt=('vsl_dwt','median'), fuel_consumption_mt=('fuel_consumption_mt', 'median'),
         cargo_value=('cargo_value','median'), canal_fee=('canal_fee','median')
         )

)

# Create binary variables for region_pairs
region_pairs_dummies = pd.get_dummies(Bulk_PA_ML_5['region_pairs'])
region_pairs_dummies = region_pairs_dummies.astype(int)  # Convert True/False to 1/0
Bulk_PA_ML_5 = pd.concat([Bulk_PA_ML_5, region_pairs_dummies], axis=1)

# Create dry or rain season indicator
# Create new columns and initialize with 0
Bulk_PA_ML_5['dry_season'] = 0
Bulk_PA_ML_5['rain_season_1st_half'] = 0
Bulk_PA_ML_5['rain_season_2nd_half'] = 0

# Define the months for each season
dry_months = [1, 2, 3, 4]
rain_1st_half_months = [5, 6, 7, 8]
rain_2nd_half_months = [9, 10, 11, 12]
# Source: https://climateknowledgeportal.worldbank.org/country/panama/climate-data-historical

# Assign 1 to the corresponding season column based on the month in proxy_transit_month
Bulk_PA_ML_5.loc[Bulk_PA_ML_5['proxy_transit_month'].dt.month.isin(dry_months), 'dry_season'] = 1
Bulk_PA_ML_5.loc[Bulk_PA_ML_5['proxy_transit_month'].dt.month.isin(rain_1st_half_months), 'rain_season_1st_half'] = 1
Bulk_PA_ML_5.loc[Bulk_PA_ML_5['proxy_transit_month'].dt.month.isin(rain_2nd_half_months), 'rain_season_2nd_half'] = 1

Bulk_PA_ML_5

# Bulk_PA_ML_5 to parquet
Bulk_PA_ML_5.to_parquet('Bulk_PA_ML_5.parquet')

"""## **Model 6** Monthly transits per commodity group"""

# Group & aggregate data
Bulk_PA_ML_6 = (
    Bulk_PA
    .groupby(['proxy_transit_month', 'commodity_group_top5'], as_index=False)
    .agg(voy_intake_sum=('voy_intake_mt','sum'),
         number_transits=('load_start_date','count'),  # load_start_date is really precise and therefore \
        # it is very unlikely that two vessels have the same load date considering the variable has minutes and seconds
        ONI=('ONI','mean'), ONI_lag_3=('ONI_lag_3','mean'), water_level_m=('water_level_m','mean'),
         water_level_m_lag_3=('water_level_m_lag_3','mean'), IFO380=('IFO380','mean'),
         Coal_Australian=('Coal_Australian','mean'), Soybeans=('Soybeans','mean'), Maize_Corn=('Maize_Corn','mean'),
         Wheat_HRW=('Wheat_HRW','mean'), Phosphate_rock=('Phosphate_rock','mean'),
         Copper=('Copper','mean'), voy_sea_duration_h=('voy_sea_duration_h','mean'),
         distance_94=('distance_94','mean')
         )

)

# Create dry or rain season indicator
# Create new columns and initialize with 0
Bulk_PA_ML_6['dry_season'] = 0
Bulk_PA_ML_6['rain_season_1st_half'] = 0
Bulk_PA_ML_6['rain_season_2nd_half'] = 0

# Define the months for each season
dry_months = [12, 1, 2, 3]
rain_1st_half_months = [4, 5, 6, 7]
rain_2nd_half_months = [8, 9, 10, 11]

# Assign 1 to the corresponding season column based on the month in proxy_transit_month
Bulk_PA_ML_6.loc[Bulk_PA_ML_6['proxy_transit_month'].dt.month.isin(dry_months), 'dry_season'] = 1
Bulk_PA_ML_6.loc[Bulk_PA_ML_6['proxy_transit_month'].dt.month.isin(rain_1st_half_months), 'rain_season_1st_half'] = 1
Bulk_PA_ML_6.loc[Bulk_PA_ML_6['proxy_transit_month'].dt.month.isin(rain_2nd_half_months), 'rain_season_2nd_half'] = 1
Bulk_PA_ML_6

# Bulk_PA_ML_6 to parquet
#Bulk_PA_ML_6.to_parquet('Bulk_PA_ML_6.parquet')

"""# **Further Analysis**"""

# Define commodity groups
commodity_groups = ['Grain', 'Bulk', 'Steels', 'Fertilizers', 'Coal', 'Others']

# Create a figure and axes for the subplots
fig, axes = plt.subplots(nrows=len(commodity_groups), ncols=1, figsize=(12, 6 * len(commodity_groups)))

# Iterate through commodity groups
for i, commodity_group in enumerate(commodity_groups):
    # Filter data for the current commodity group
    filtered_data = Bulk_PA[Bulk_PA['commodity_group_top5'] == commodity_group]

    # Group data by year and month and count transits
    # Rename the 'proxy_transit_month' column in the groupby to avoid conflict
    monthly_data = filtered_data.groupby([filtered_data['proxy_transit_month'].dt.year.rename('year'), filtered_data['proxy_transit_month'].dt.month.rename('month')])['proxy_transit_month'].count().reset_index(name='transit_count')

    # Recreate the 'proxy_transit_month' column from 'year' and 'month'
    monthly_data['proxy_transit_month'] = pd.to_datetime(monthly_data['year'].astype(str) + '-' + monthly_data['month'].astype(str), format='%Y-%m')

    # Create the monthly chart using seaborn
    sns.lineplot(x='month', y='transit_count', hue='year', data=monthly_data, ax=axes[i], marker='o', palette="bright")

    # Set title and labels
    axes[i].set_title(f'Monthly Variation of Transits for {commodity_group}')
    axes[i].set_xlabel('Month')
    axes[i].set_ylabel('Transit Count')

# Adjust layout and display the charts
plt.tight_layout()
plt.show()

# Define region pairs (replace with your actual region pairs)
region_pairs = Bulk_PA['region_pairs'].unique()

# Create a figure and axes for the subplots
fig, axes = plt.subplots(nrows=len(region_pairs), ncols=1, figsize=(12, 6 * len(region_pairs)))

# Iterate through region pairs
for i, region_pair in enumerate(region_pairs):
    # Filter data for the current region pair
    filtered_data = Bulk_PA[Bulk_PA['region_pairs'] == region_pair]

    # Group data by year and month and count transits
    monthly_data = filtered_data.groupby([filtered_data['proxy_transit_month'].dt.year.rename('year'), filtered_data['proxy_transit_month'].dt.month.rename('month')])['proxy_transit_month'].count().reset_index(name='transit_count')

    # Recreate the 'proxy_transit_month' column from 'year' and 'month'
    monthly_data['proxy_transit_month'] = pd.to_datetime(monthly_data['year'].astype(str) + '-' + monthly_data['month'].astype(str), format='%Y-%m')

    # Create the monthly chart using seaborn
    sns.lineplot(x='month', y='transit_count', hue='year', data=monthly_data, ax=axes[i], marker='o', palette="bright")

    # Set title and labels
    axes[i].set_title(f'Monthly Variation of Transits for {region_pair}')
    axes[i].set_xlabel('Month')
    axes[i].set_ylabel('Transit Count')

plt.tight_layout()
plt.show()

# Assuming IFO380_19_24 is a DataFrame with 'Year_Month' and 'IFO380' columns
plt.figure(figsize=(12, 6))

# Convert 'Year_Month' to datetime before plotting
IFO380_19_24['Year_Month'] = IFO380_19_24['Year_Month'].dt.to_timestamp()

sns.lineplot(x='Year_Month', y='IFO380', data=IFO380_19_24, marker='o')

# Add vertical line for September 2019
plt.axvline(pd.to_datetime('2019-10-01'), color='grey', linestyle='--', label='IFO380 bunker price introduction')


plt.title('IFO380 Price Over Time')
plt.xlabel('Year-Month')
plt.ylabel('IFO380 Price')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

# Assuming Bulk_PA is your DataFrame
plt.figure(figsize=(10, 6))
sns.histplot(Bulk_PA['vsl_dwt'][Bulk_PA['vsl_dwt']<= 100000], bins=20, kde=True)
plt.title('Distribution of DWT')
plt.xlabel('DWT')
plt.ylabel('Frequency')
plt.show()

# Assuming Bulk_PA is your DataFrame
commodity_groups = Bulk_PA['commodity_group_top5'].unique()

# Set up subplots in a 3x2 grid
num_cols = 2
num_rows = int(np.ceil(len(commodity_groups) / num_cols))
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 10), sharex=True)

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Loop through each commodity group and create histogram
for i, group in enumerate(commodity_groups):
    # Filter data for DWT less than 100,000
    filtered_data = Bulk_PA[(Bulk_PA['commodity_group_top5'] == group) & (Bulk_PA['vsl_dwt'] <= 100000)]

    ax = sns.histplot(filtered_data['vsl_dwt'],
                      bins=20, kde=True, stat="count", ax=axes[i])

    axes[i].set_title(f'Distribution of DWT for {group} (DWT <= 100,000)')
    axes[i].set_xlabel('DWT')
    axes[i].set_ylabel('Number of Ships')

# Hide any unused subplots
for j in range(len(commodity_groups), len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

# Distribution of the number of transits per commodity
# List commodities and their groups, then display the top 10
#filter Bulk Pa before june 2023
Bulk_PA_before_june = Bulk_PA[Bulk_PA['proxy_transit_month'] < pd.Period('2023-06', freq='M')]
commodity_group_counts = Bulk_PA_before_june.groupby(['commodity_group', 'commodity'])['commodity'].count().reset_index(name='count')

# Sort by count in descending order, take the top 10, and reset the index
top_10_commodities = commodity_group_counts.sort_values('count', ascending=False).head(20).reset_index(drop=True)

# Calculate total commodity count in Bulk_PA
total_commodity_count = Bulk_PA_before_june['commodity'].count()

# Add a percentage column
top_10_commodities['percentage'] = (top_10_commodities['count'] / total_commodity_count * 100).map('{:.2f}%'.format)

# Calculate cumulative percentage
top_10_commodities['cumulative_percentage'] = top_10_commodities['count'].cumsum() / total_commodity_count * 100
top_10_commodities['cumulative_percentage'] = top_10_commodities['cumulative_percentage'].map('{:.2f}%'.format)


# Display the top 10 with percentage and cumulative percentage
display(top_10_commodities)

# Distribution of the number of transits per commodity
# List commodities and their groups, then display the top 10
#filter Bulk Pa before june 2023
Bulk_PA_after_june = Bulk_PA[Bulk_PA['proxy_transit_month'] >= pd.Period('2023-06', freq='M')]
commodity_group_counts = Bulk_PA_after_june.groupby(['commodity_group', 'commodity'])['commodity'].count().reset_index(name='count')

# Sort by count in descending order, take the top 10, and reset the index
top_10_commodities = commodity_group_counts.sort_values('count', ascending=False).head(20).reset_index(drop=True)

# Calculate total commodity count in Bulk_PA
total_commodity_count = Bulk_PA_after_june['commodity'].count()

# Add a percentage column
top_10_commodities['percentage'] = (top_10_commodities['count'] / total_commodity_count * 100).map('{:.2f}%'.format)

# Calculate cumulative percentage
top_10_commodities['cumulative_percentage'] = top_10_commodities['count'].cumsum() / total_commodity_count * 100
top_10_commodities['cumulative_percentage'] = top_10_commodities['cumulative_percentage'].map('{:.2f}%'.format)


# Display the top 10 with percentage and cumulative percentage
display(top_10_commodities)