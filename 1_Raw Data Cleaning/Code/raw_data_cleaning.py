# -*- coding: utf-8 -*-
"""Raw Data Cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19BYS6fwnhLpZa2GS9bWQ6UEGoobx0Y7u

This script imports all of the data needed for the thesis. It proceeds to the cleaning and create a single data frame that contains all the relevant variables.

Most of the script at the beginning was converted as comments.
It was used for the initial import of the data. After the initial import, we did not need it, and used the data frame directly.
"""

#Library import
import pyarrow as pa
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import statistics
import numpy as np
from datetime import datetime
import requests
from io import StringIO
import matplotlib.ticker as mticker
import numpy as np

"""# **AXS Marine dataset**

First import of the raw data - Bulk carrier transits through the Panama Canal from 01-2019 - 04-2025 -Data source: AXS Marine (AXS Dry)- waypoint filter: Panama Canal.
"""

## Read commodities_bulk
#commodities_bulk_1 = pd.read_parquet('commodities_bulk_01_01_2019_30_06_22.parquet')
#commodities_bulk_2 = pd.read_parquet('commodities_bulk_01_07_2022_05_04_25.parquet')
#pd.set_option("display.max_columns", None) # Show all colum
#commodities_bulk_1.head(4)

#Bulk_PA = pd.concat([commodities_bulk_1, commodities_bulk_2], ignore_index=True)

# Bulk_PA to parquet
#Bulk_PA = Bulk_PA.to_parquet('Bulk_PA.parquet')

"""# **Fuel dataset**

First import of the raw data - Fuel consumption and distance data of Bulk carriers that transited through the Panama Canal from 01-2019 - 01-2025 -Data source: AIS data
"""

## Import fuel consumption dataset
#import glob

#file_paths = glob.glob('fuel_*.csv')
#dfs = []

#for file_path in file_paths:
 #   df = pd.read_csv(file_path)
  #  dfs.append(df)

#fuel = pd.concat(dfs, ignore_index=True)

#display(fuel)

## Change '/' for '-' in discharge_start_date	load_end_date
#fuel['discharge_start_date'] = fuel['discharge_start_date'].str.replace('/', '-')
#fuel['load_end_date'] = fuel['load_end_date'].str.replace('/', '-')
## Discharge date to datetime %Y-%m
## Handle errors during conversion
#for i in range(len(fuel)):
 #   try:
  #      fuel.loc[i, 'discharge_start_date'] = pd.to_datetime(fuel.loc[i, 'discharge_start_date'], format='%Y-%m-%d')
   # except ValueError:
    #    try:  # Try alternative format in the except block
     #       fuel.loc[i, 'discharge_start_date'] = pd.to_datetime(fuel.loc[i, 'discharge_start_date'], format='%d-%m-%Y')
      #  except ValueError:
       #     fuel.loc[i, 'discharge_start_date'] = pd.NaT
## For load_end_date
#for i in range(len(fuel)):
 #   try:
  #      fuel.loc[i, 'load_end_date'] = pd.to_datetime(fuel.loc[i, 'load_end_date'], format='%Y-%m-%d')
  #  except ValueError:
   #     try:  # Try alternative format in the except block
    #        fuel.loc[i, 'load_end_date'] = pd.to_datetime(fuel.loc[i, 'load_end_date'], format='%d-%m-%Y')
     #   except ValueError:
      #      fuel.loc[i, 'load_end_date'] = pd.NaT


#fuel

# Convert 'discharge_start_date' to datetime, forcing errors to NaT
#fuel['discharge_start_date'] = pd.to_datetime(fuel['discharge_start_date'], errors='coerce')
#fuel['load_end_date'] = pd.to_datetime(fuel['load_end_date'], errors='coerce')
#fuel

# Fuel to parquet
#fuel= fuel.to_parquet('Fuel_consumption.parquet')

## Discharge start date to daytime
#fuel['discharge_start_date'] = pd.to_datetime(fuel['discharge_start_date'])

"""Now both datasets are combined into their respective .parquet files to avoid importing all raw datasets everytime and make further manipulation easier.

## **Fuel consumption dataset**

Upload the fuel consumption and distance dataset
"""

# Read fuel-consumption parquet
Fuel_consumption = pd.read_parquet('Fuel_consumption.parquet')
Fuel_consumption

# Create fuel consumption mt column
Fuel_consumption['fuel_consumption_mt'] = (Fuel_consumption['me_con_g'] + Fuel_consumption['ae_con_g'] + Fuel_consumption['ab_con_g'])/ 1000000 # grams to metric tons
Fuel_consumption

"""# **Bulk_PA dataset**

Upload the bulk carrier transits dataset
"""

# Read Bulk_PA from parquet
Bulk_PA_original_data = pd.read_parquet('Bulk_PA.parquet')
# Set pd to see all columns
pd.set_option('display.max_columns', None)
Bulk_PA_original_data

# See all the column names
Bulk_PA_original_data.columns

# Create a new dataframe keeping only relevant columns from Bulk_PA_original_data df
Bulk_PA = Bulk_PA_original_data[['load_zone', 'load_country', 'load_port', 'load_draft', 'load_start_date',
                                 'load_end_date', 'discharge_zone', 'discharge_country', 'discharge_port', 'discharge_draft',
                                 'discharge_start_date', 'discharge_end_date', 'commodity', 'commodity_group','vsl_imo',
                                 'vsl_dwt', 'vsl_name','vsl_type', 'vsl_max_speed', 'vsl_max_draft', 'has_part_voy',
                                 'voy_draft_diff', 'voy_load_draft_ratio', 'voy_intake_mt','voy_avg_speed', 'voy_speed_ratio',
                                 'voy_top_speed', 'voy_duration', 'voy_sea_duration', 'voy_ais_destination', 'blackout_duration']]

# Change column name, vsl_imo to imo
Bulk_PA = Bulk_PA.rename(columns={'vsl_imo': 'imo'})

# Convert duration from minutes to hours
Bulk_PA['voy_duration'] = Bulk_PA['voy_duration'] / 60
Bulk_PA['voy_sea_duration'] = Bulk_PA['voy_sea_duration'] / 60
Bulk_PA['blackout_duration'] = Bulk_PA['blackout_duration'] / 60

# Rename the variables _h
Bulk_PA = Bulk_PA.rename(columns={'voy_duration': 'voy_duration_h'})
Bulk_PA = Bulk_PA.rename(columns={'voy_sea_duration': 'voy_sea_duration_h'})
Bulk_PA = Bulk_PA.rename(columns={'blackout_duration': 'blackout_duration_h'})

# Remove duplicates based on imo, load_end_date, and discharge_start_date â€” keep the first occurrence
Bulk_PA = Bulk_PA.drop_duplicates(subset=['imo', 'load_end_date', 'discharge_start_date'], keep='first').reset_index(drop=True)

print("Duplicates removed. Only one entry per unique (imo, load_end_date, discharge_start_date) remains.")

# Cut data to have 2019-2025 to keep the same time frame for both Bulk and Fuel datasets
# Convert load start date and discharge end date to datetime
Bulk_PA['load_start_date'] = pd.to_datetime(Bulk_PA['load_start_date'], errors='coerce')
Bulk_PA['discharge_end_date'] = pd.to_datetime(Bulk_PA['discharge_end_date'], errors='coerce')
# Filter data
Bulk_PA = Bulk_PA[Bulk_PA['load_start_date'] >= '2019-01-01']
Bulk_PA = Bulk_PA[Bulk_PA['discharge_end_date'] < '2025-01-01']
# Reset index
Bulk_PA = Bulk_PA.reset_index(drop=True)

Bulk_PA.describe()

Bulk_PA

"""## **Merge Bulk and Fuel dataframes**

Merge both dataframes by matching the IMO number, the load_end_date and the discharge_start_date in both datasets.
"""

# Merge the DataFrames using a left join
Bulk_PA = Bulk_PA.merge(
    Fuel_consumption,
    on=['imo', 'load_end_date', 'discharge_start_date'],
    how='left'
)

print("DataFrames merged successfully.")

# Rename the distance column
Bulk_PA = Bulk_PA.rename(columns={'distance_nm': 'distance'})

# Calcute the number of missing values for the distance variable for each month
# Count NaN values in 'distance' column
nan_count = Bulk_PA['distance'].isna().sum()

# Print the result
print(f"Number of NaN values in 'distance' column: {nan_count}")

# Create a DataFrame with the missing values
missing_distance_df = Bulk_PA[Bulk_PA['distance'].isna()]

# Group by discharge_start_date and count missing values
missing_by_month = missing_distance_df.groupby(missing_distance_df['discharge_start_date'].dt.to_period('M'))['imo'].count().reset_index()

# Rename columns for clarity
missing_by_month = missing_by_month.rename(columns={'discharge_start_date': 'Month', 'imo': 'Missing_Count'})

# Add a total row
total_row = pd.DataFrame({'Month': ['Total'], 'Missing_Count': [missing_by_month['Missing_Count'].sum()]})
missing_by_month = pd.concat([missing_by_month, total_row], ignore_index=True)

# Display the result
display(missing_by_month)

# Analyse distances that equal to 0
# Count rows where 'distance' is 0
zero_count = Bulk_PA[Bulk_PA['distance'] == 0]['distance'].count()

# Print the result
print(f"Number of rows with distance = 0: {zero_count}")

# Create a DataFrame with the zero distance values
zero_distance_df = Bulk_PA[Bulk_PA['distance'] == 0]

# Group by discharge_start_date and count zero distance values
zero_by_month = zero_distance_df.groupby(zero_distance_df['discharge_start_date'].dt.to_period('M'))['imo'].count().reset_index()

# Rename columns for clarity
zero_by_month = zero_by_month.rename(columns={'discharge_start_date': 'Month', 'imo': 'Zero_Count'})

# Add a total row
total_row = pd.DataFrame({'Month': ['Total'], 'Zero_Count': [zero_by_month['Zero_Count'].sum()]})
zero_by_month = pd.concat([zero_by_month, total_row], ignore_index=True)

# Display the result
display(zero_by_month)

"""Create a new variable to flag when the distance is an outlier in the Fuel_Consumption dataset"""

# Test to validate the expected value of distance based on the variables in Bulk_PA from AXS Marine
# As the distance is extracted from AIS

# Create a proxy for the distance based on the Bulk dataset
Bulk_PA['proxy_distance'] = Bulk_PA['voy_avg_speed'] * Bulk_PA['voy_sea_duration_h']
# Measure the difference between the proxy and the distance
Bulk_PA['distance_difference'] = Bulk_PA['distance'] - Bulk_PA['proxy_distance']

"""## **Bulk_PA data cleaning**

The variables are analysed. When there are outliers, we analyse if we keep the information as it is or if it needs filtering.
"""

# Set the code so that we will see all the columns of our data frame
pd.set_option('display.max_columns', None)

"""First exploration of our dataset

### **Voyage duration and voyage sea duration**

The voyage duration includes the loading and unloading operations. The sea voyage duration only accounts for the sailing portion of the voyage.
"""

# Box plot of voyage_sea_duration
plt.figure(figsize=(10, 6))
sns.boxplot(x=Bulk_PA['voy_duration_h'])
plt.title('Box Plot of Voyage Duration (h)')
plt.xlabel('Voyage Duration (h)')
plt.show()

# Closer look to the extremes of voyage duration time
top_1_percent_voy_duration_h = Bulk_PA[Bulk_PA['voy_duration_h'] < Bulk_PA['voy_duration_h'].quantile(0.01)]
# Sort 'top_1_percent_voy_duration_h' by 'voy_duration_h' in descending order
top_1_percent_voy_duration_h_sorted = top_1_percent_voy_duration_h.sort_values(
    by=['voy_duration_h'], ascending=False
)

# Display the sorted DataFrame
display(top_1_percent_voy_duration_h_sorted)

# Sea voyage duration box plot
# Box plot of voyage_sea_duration_h
plt.figure(figsize=(10, 6))
sns.boxplot(x=Bulk_PA['voy_sea_duration_h'])
plt.title('Box Plot of Voyage Sea Duration (h)')
plt.xlabel('Voyage Duration (h)')
plt.show()

"""### **Blackout duration**"""

# Blackout duration box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x=Bulk_PA['blackout_duration_h'])
plt.title('Box Plot of Blackout Duration (h)')
plt.xlabel('Blackout Duration (h)')
plt.show()

# Count rows where 'blackout_duration_h' is greater than 0
blackout_count = Bulk_PA[Bulk_PA['blackout_duration_h'] > 0]['blackout_duration_h'].count()

# Print the blackout_count
print(f"Number of rows with blackout_duration_h > 0: {blackout_count}")

# Closer look to the extremes of blackout duration
top_1_percent_blackout_duration_h = Bulk_PA[Bulk_PA['blackout_duration_h'] > Bulk_PA['blackout_duration_h'].quantile(0.99)]
# Sort 'top_1_percent_blackout_duration_h' by 'blackout_duration_h' in descending order
top_1_percent_blackout_duration_h_sorted = top_1_percent_blackout_duration_h.sort_values(
    by=['blackout_duration_h'], ascending=False
)

# Display the sorted DataFrame
display(top_1_percent_blackout_duration_h_sorted)

"""The blackout duration is not the most important variable for our analysis and it does not help us identify the outliers. We will focus on voyage sea duration and distance 94.

### **Distance**

Merged data from AIS containting the distance variable has some high extremes. We changed them by calculating a distance proxy using average speed and voyage sea duration, to then switch the top and bottom 6% to that proxy if it was not larger (lower) than these extreme values. The 6% was based on the analysis of the difference between the distance and the proxy distance. In that interval, the difference remained negative for the bottom 6% and positive for the top 6% meaning that the proxy was a better estimate than the AIS distance.
This analysis was performed to avoid deleting too many rows.
Changing the value of the distance was performed to delete outliers while keeping the information from the voyages. The distance was greater then one world tour or close to 0 in some cases, which is not rational.
"""

# Distance box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x=Bulk_PA['distance'])
plt.title('Box Plot of Distance')
plt.xlabel('Distance in nm')
plt.show()

# Closer look to the extremes of distance values
# The 6 % threshold was chosen after testing the values 1 to 10.


top_6_percent_distance = Bulk_PA[Bulk_PA['distance'] > Bulk_PA['distance'].quantile(0.94)]
# Sort 'top_6_percent_distance' by 'distance' in descending order
top_6_percent_distance_sorted = top_6_percent_distance.sort_values(
    by=['distance'], ascending=False)

# Display the sorted DataFrame
display(top_6_percent_distance_sorted)

# Replace the top and bottom 6% of distance with proxy distance

# Create a new column 'distance_94' and initialize it with NaN
# 'distance_94' is to keep a trace of the change and 94 comes from 100% - 6% threshold to delete outliers.
# In the end, we keep 88% of the original distance variable, and modify 12% to the proxi distance
Bulk_PA['distance_94'] = Bulk_PA['distance']

# Ensure 'distance_94' has a float data type to accommodate potential Proxy_distance values
Bulk_PA['distance_94'] = Bulk_PA['distance'].astype(float)

# Assign values from 'distance' to 'distance_94' for the top & bottom 6% of the values
Bulk_PA.loc[
    (Bulk_PA['distance'] > Bulk_PA['distance'].quantile(0.94)) & (Bulk_PA['distance'] > Bulk_PA['proxy_distance']),
    'distance_94',
] = Bulk_PA['proxy_distance']

Bulk_PA.loc[
    (Bulk_PA['distance'] < Bulk_PA['distance'].quantile(0.06)) & (Bulk_PA['distance'] < Bulk_PA['proxy_distance']),
    'distance_94',
] = Bulk_PA['proxy_distance']

#Add proxy distance when distance is == 0
Bulk_PA.loc[Bulk_PA['distance'] == 0, 'distance_94'] = Bulk_PA['proxy_distance']

#Add proxy distance when distance is == NaN
Bulk_PA.loc[Bulk_PA['distance'].isna(), 'distance_94'] = Bulk_PA['proxy_distance']

display(Bulk_PA)

# Distance_94 box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x=Bulk_PA['distance_94'])
plt.title('Box Plot of Distance_94')
plt.xlabel('Distance in nm')
plt.show()

"""Note that some transits have unexpected values that needs deeper cleaning.
For example, Lucy J vessel has a small distance and a small voy duration which doesn't correspond to a realistic distance and duration between Panama and Romania voyage.
To identify these issues, we will develop a region analysis.
"""

# Example of problematic transits
display(Bulk_PA[Bulk_PA['vsl_name'] == 'LUCY J'])

"""### **Regions**

Create regions to better estimate the errors in distance and voyage duration.
Note that these regions will be further used to model the number of monthly transits

AXS provides the ports and zones information. We want to flag the outlier distances by making sure the distance between two regions respects what is expected from normal sailing conditions
"""

# Identify the load zones defined by AXS Marine database
load_zone_counts = Bulk_PA['load_zone'].value_counts().reset_index()
load_zone_counts.columns = ['load_zone', 'count']
load_zone_counts_sorted = load_zone_counts.sort_values(by=['load_zone'])
load_zone_counts_sorted = load_zone_counts_sorted.reset_index(drop=True)
display(load_zone_counts_sorted)

# Identify the discharge zones defined by AXS Marine database
discharge_zone_counts = Bulk_PA['discharge_zone'].value_counts().reset_index()
discharge_zone_counts.columns = ['discharge_zone', 'count']
discharge_zone_counts_sorted = discharge_zone_counts.sort_values(by=['discharge_zone'])
discharge_zone_counts_sorted = discharge_zone_counts_sorted.reset_index(drop=True)
display(discharge_zone_counts_sorted)

# Define nine broad load zone regions based on the zones from AXS
# Define a function to create binary columns to identify the load zone region
def create_load_zone_column(df, new_column_name, load_zones):
    df[new_column_name] = 0
    df.loc[df['load_zone'].isin(load_zones), new_column_name] = 1
    return df

# Group the load zones to creat broad load zone regions
load_zones_NE_America = ['USG', 'Carribbean', 'East Coast Canada', 'East Coast Central America', 'East Coast U.S', 'Great Lakes', 'Saint Lawrence']
load_zones_NW_America = ['NoPac', 'West Coast Central America']
load_zones_SE_America = ['East Coast South America', 'North Coast South America']
load_zones_SW_America = ['South Pacific', 'West Coast South America']
load_zones_E_Asia = ['South East Asia', 'Far East']
load_zones_W_Asia = ['Arabian Gulf', 'East Coast India', 'West Coast India']
load_zones_Europe_N_Africa = ['Antwerp Rotterdam Amsterdam Ghent', 'Baltic', 'Black Sea', 'East Mediterranean', 'North Continent', 'North West Africa', 'Spain Atlantic', 'United Kingdom Ireland', 'West Mediterranean']
load_zones_Australia = ['East Aussie', 'New Zealand', 'West Aussie']
load_zones_S_Africa = ['South Africa', 'South West Africa', 'East Africa']

# Create the new columns for each load zone region
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_NE_America', load_zones_NE_America)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_NW_America', load_zones_NW_America)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_SE_America', load_zones_SE_America)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_SW_America', load_zones_SW_America)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_E_Asia', load_zones_E_Asia)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_W_Asia', load_zones_W_Asia)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_Europe_N_Africa', load_zones_Europe_N_Africa)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_Australia', load_zones_Australia)
Bulk_PA = create_load_zone_column(Bulk_PA, 'load_S_Africa', load_zones_S_Africa)

# Definition of the nine broad discharge zone regions (same as the broad load zone regions)
# Define a function to create binary columns to identify the discharge zone region
def create_discharge_zone_column(df, new_column_name, discharge_zones):
    df[new_column_name] = 0
    df.loc[df['discharge_zone'].isin(discharge_zones), new_column_name] = 1
    return df

# Group the discharge zones to creat broad discharge zone regions
discharge_zones_NE_America = ['USG', 'Carribbean', 'East Coast Canada', 'East Coast Central America', 'East Coast U.S', 'Great Lakes', 'Saint Lawrence']
discharge_zones_NW_America = ['NoPac', 'West Coast Central America']
discharge_zones_SE_America = ['East Coast South America', 'North Coast South America']
discharge_zones_SW_America = ['South Pacific', 'West Coast South America']
discharge_zones_E_Asia = ['South East Asia', 'Far East']
discharge_zones_W_Asia = ['Arabian Gulf', 'East Coast India', 'West Coast India', 'Red Sea']
discharge_zones_Europe_N_Africa = ['Antwerp Rotterdam Amsterdam Ghent', 'Baltic', 'Black Sea', 'East Mediterranean', 'North Continent', 'North West Africa', 'Spain Atlantic', 'United Kingdom Ireland', 'West Mediterranean', 'French Atlantic']
discharge_zones_Australia = ['East Aussie', 'New Zealand', 'West Aussie']
discharge_zones_S_Africa = ['South Africa', 'South West Africa', 'East Africa']

# Create the new columns for each discharge zone region
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_NE_America', discharge_zones_NE_America)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_NW_America', discharge_zones_NW_America)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_SE_America', discharge_zones_SE_America)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_SW_America', discharge_zones_SW_America)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_E_Asia', discharge_zones_E_Asia)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_W_Asia', discharge_zones_W_Asia)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_Europe_N_Africa', discharge_zones_Europe_N_Africa)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_Australia', discharge_zones_Australia)
Bulk_PA = create_discharge_zone_column(Bulk_PA, 'disch_S_Africa', discharge_zones_S_Africa)


Bulk_PA

# Broad illustration of the regions
import matplotlib.pyplot as plt
from shapely.geometry import box
import matplotlib.patches as mpatches
import geopandas as gpd
import requests
import zipfile
import io

#from geopandas_data import natural_earth
url = "https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip"
world = gpd.read_file(url)


# Define approximate bounding boxes for each region
regions = {
    'NE America': box(-100, 10, -50, 60),
    'NW America': box(-180, -10, -100, 60),
    'SE America': box(-65, -40, -30, 10),
    'SW America': box(-100, -60, -65, 10),
    'W Asia': box(50, 10, 90, 35),
    'E Asia': box(90, -10, 180, 60),
    'Europe & N Africa': box(-30, 10, 50, 70),
    'Australia': box(110, -50, 180, -10),
    'S Africa': box(-20, -40, 60, 10),
}

# Choose a color palette
colors = plt.get_cmap('tab10').colors
region_colors = {name: colors[i % len(colors)] for i, name in enumerate(regions)}

# Plot the base world map
fig, ax = plt.subplots(figsize=(15, 10))
world.plot(ax=ax, color='lightgrey', edgecolor='white')

# Overlay each region
for name, geom in regions.items():
    gpd.GeoSeries([geom]).plot(ax=ax, color=region_colors[name], alpha=0.4)

# Create legend patches
patches = [mpatches.Patch(color=region_colors[name], label=name) for name in regions]
ax.legend(handles=patches, loc='lower left', fontsize='medium', frameon=False)

# Final styling
ax.set_axis_off()
ax.set_title('World map with 9 custom load zone regions', fontsize=16)
plt.show()

# Verification: Make sure that each transit is associated with exactly one load region and one discharge region
# Sum of the relevant columns
columns_to_sum = [
    'load_NE_America', 'load_NW_America', 'load_SE_America', 'load_SW_America', 'load_E_Asia',
    'load_W_Asia', 'load_Europe_N_Africa', 'load_Australia', 'load_S_Africa',
    'disch_NE_America', 'disch_NW_America', 'disch_SE_America', 'disch_SW_America', 'disch_E_Asia',
    'disch_W_Asia', 'disch_Europe_N_Africa', 'disch_Australia', 'disch_S_Africa'
]

# Create the 'verification' column by summing the specified columns (has to be equal to 2 for every row)
Bulk_PA['verification'] = Bulk_PA[columns_to_sum].sum(axis=1)

# Verify that the values are equal to 2
display(Bulk_PA['verification'].describe())

"""All the zones are associated to one region!"""

# Drop the verification column
Bulk_PA = Bulk_PA.drop(columns=['verification'])

# Distribution of load and discharge regions
# Count the number of transits in each load and discharge regions
columns_to_count = [
    'load_NE_America', 'load_NW_America', 'load_SE_America', 'load_SW_America', 'load_E_Asia',
    'load_W_Asia', 'load_Europe_N_Africa', 'load_Australia', 'load_S_Africa',
    'disch_NE_America', 'disch_NW_America', 'disch_SE_America', 'disch_SW_America', 'disch_E_Asia',
    'disch_W_Asia', 'disch_Europe_N_Africa', 'disch_Australia', 'disch_S_Africa'
]

# Create a dictionary to store the counts
value_counts = {}

# Loop through the columns and count the occurrences of '1'
for column in columns_to_count:
    value_counts[column] = Bulk_PA[column].sum()

# Print the number of transits of each load and discharge region
for column, count in value_counts.items():
    display(f"Number of '1's in {column}: {count}")

# Define a function to create the OD_region value where the order matters. We make a distinction between load and discharge zones
def get_od_region(row):
    load_regions = [
        'load_NE_America', 'load_NW_America', 'load_SE_America', 'load_SW_America', 'load_E_Asia',
        'load_W_Asia', 'load_Europe_N_Africa', 'load_Australia', 'load_S_Africa'
    ]
    disch_regions = [
        'disch_NE_America', 'disch_NW_America', 'disch_SE_America', 'disch_SW_America', 'disch_E_Asia',
        'disch_W_Asia', 'disch_Europe_N_Africa', 'disch_Australia', 'disch_S_Africa'
    ]

    load_region = next((region[5:] for region in load_regions if row[region] == 1), None)
    disch_region = next((region[6:] for region in disch_regions if row[region] == 1), None)

    if load_region and disch_region:
        return f"{load_region}_to_{disch_region}"
    else:
        return None

# Apply the function to create the 'OD_region' column
Bulk_PA['OD_region'] = Bulk_PA.apply(get_od_region, axis=1)

# Define a function to create the regions_pairs value where the order doesn't matter to have an estimate of the average distance between two regions
# It will help us to flag the outliers
def get_pair_region(row):
    load_regions = [
        'load_NE_America', 'load_NW_America', 'load_SE_America', 'load_SW_America', 'load_E_Asia',
        'load_W_Asia', 'load_Europe_N_Africa', 'load_Australia', 'load_S_Africa'
    ]
    disch_regions = [
        'disch_NE_America', 'disch_NW_America', 'disch_SE_America', 'disch_SW_America', 'disch_E_Asia',
        'disch_W_Asia', 'disch_Europe_N_Africa', 'disch_Australia', 'disch_S_Africa'
    ]

    load_region = next((region[5:] for region in load_regions if row[region] == 1), None)
    disch_region = next((region[6:] for region in disch_regions if row[region] == 1), None)

    if load_region and disch_region:
        # Sort the regions alphabetically
        regions = sorted([load_region, disch_region])
        return f"{regions[0]}-{regions[1]}"
    else:
        return None

# Apply the function to create the 'region_pairs' column
Bulk_PA['region_pairs'] = Bulk_PA.apply(get_pair_region, axis=1)

# List the unique region pairs

# Count occurrences of each unique value in 'region_pairs'
region_pair_counts = Bulk_PA['region_pairs'].value_counts()

# Calculate the total sum : Number of transits for each region pairs
total_sum = region_pair_counts.sum()

# Display the counts and total sum
print(region_pair_counts)
print(f"\nTotal sum: {total_sum}")

"""As seen above, most of the transits were between East Asia and North East America. The top nine regions pairs contain almost all the transits."""

# Some region pairs have too few observations to be statistically significant - we remove them
# Count occurrences of each unique value in 'region_pairs'
region_pair_counts = Bulk_PA['region_pairs'].value_counts()

# Remove the region pairs with count lower than 100
filtered_region_pairs = region_pair_counts[region_pair_counts > 100].index

# Keep rows in Bulk_PA where 'region_pairs' is in filtered_region_pairs
Bulk_PA = Bulk_PA[Bulk_PA['region_pairs'].isin(filtered_region_pairs)]

# Reset index
Bulk_PA = Bulk_PA.reset_index(drop=True)

# Remove empty load and discharge regions columns
columns_to_check = [
    'load_NE_America', 'load_NW_America', 'load_SE_America', 'load_SW_America', 'load_E_Asia',
    'load_W_Asia', 'load_Europe_N_Africa', 'load_Australia', 'load_S_Africa',
    'disch_NE_America', 'disch_NW_America', 'disch_SE_America', 'disch_SW_America', 'disch_E_Asia',
    'disch_W_Asia', 'disch_Europe_N_Africa', 'disch_Australia', 'disch_S_Africa'
]

# Loop through the columns and remove if all values are 0
for column in columns_to_check:
    if Bulk_PA[column].sum() == 0:
        Bulk_PA = Bulk_PA.drop(columns=[column])
        print(f"Column '{column}' removed as it contained only 0 values.")

"""### **Remove outliers based on region pairs**

Remove the outliers by making sure that the origin-destination is coherent with the distance and the voyage duration. We filter the outliers by region pair.
"""

# Reduce the dataframe to keep only relevant information to identify the outliers
# Create a copy of the relevant columns from Bulk_PA
flagged_outliers = Bulk_PA[['imo', 'load_end_date', 'discharge_start_date', 'voy_sea_duration_h', 'distance', 'distance_94']].copy()

# Get unique region pairs
unique_region_pairs = Bulk_PA['region_pairs'].unique()

# Create binary columns for each region pair
for region_pair in unique_region_pairs:

    flagged_outliers[region_pair] = (Bulk_PA['region_pairs'] == region_pair).astype(int)

# Display the new DataFrame
flagged_outliers

# Voyage sea duration box plot for each region pair
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers.columns[5:]

# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers.melt(
    id_vars=['voy_sea_duration_h'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='voy_sea_duration_h', data=filtered_data, order=sorted_region_pairs)
plt.title('Voyage Sea Duration Distribution by Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Duration (h)')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

# Distance_94 box plot for each region pair
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers.columns[5:]

# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers.melt(
    id_vars=['distance_94'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='distance_94', data=filtered_data, order=sorted_region_pairs)
plt.title('Distance_94 Distribution by Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Distance')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

#Remove sea_voyage_duration outliers by region pair
def remove_outliers_by_region(df, column_name, region_column, multiplier=1.5):
    """Removes outliers from a DataFrame based on IQR for each region.

    Args:
        df: The DataFrame to modify.
        column_name: The name of the column containing the data to check for outliers.
        region_column: The name of the column containing the region information.
        multiplier: The IQR multiplier to define outliers (default: 1.5).

    Returns:
        The modified DataFrame with outliers removed.
    """

    filtered_df = df.copy()

    for region in df[region_column].unique():
        region_data = df[df[region_column] == region][column_name]
        Q1 = region_data.quantile(0.25)
        Q3 = region_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - multiplier * IQR
        upper_bound = Q3 + multiplier * IQR

        # Filter out outliers for the current region
        filtered_df = filtered_df[
            ~(
                (filtered_df[region_column] == region)
                & ((filtered_df[column_name] < lower_bound) | (filtered_df[column_name] > upper_bound))
            )
        ]

    # Reset the index of the filtered DataFrame
    filtered_df = filtered_df.reset_index(drop=True)

    return filtered_df

# Apply the function to Bulk_PA
Bulk_PA = remove_outliers_by_region(Bulk_PA, 'voy_sea_duration_h', 'region_pairs')

# Create the input for the box plot analysis (analysis of outliers removal)
# Create a copy of the relevant columns from Bulk_PA
flagged_outliers_2 = Bulk_PA[['imo', 'load_end_date', 'discharge_start_date', 'voy_sea_duration_h', 'distance', 'distance_94']].copy()

# Get unique region pairs
unique_region_pairs = Bulk_PA['region_pairs'].unique()

# Create binary columns for each region pair
for region_pair in unique_region_pairs:

    flagged_outliers_2[region_pair] = (Bulk_PA['region_pairs'] == region_pair).astype(int)

# Voyage sea duration box plot for each region pair after voyage sea duration outliers removal
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers_2.columns[5:]
# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers_2.melt(
    id_vars=['voy_sea_duration_h'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='voy_sea_duration_h', data=filtered_data, order=sorted_region_pairs)
plt.title('Voyage Sea Duration Distribution by Region Pair After Outliers Removal')
plt.xlabel('Region Pair')
plt.ylabel('Duration (h)')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

# Distance_94 box plot for each region pair after voyage sea duration outliers removal
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers_2.columns[5:]
# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers_2.melt(
    id_vars=['distance_94'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='distance_94', data=filtered_data, order=sorted_region_pairs)
plt.title('Distance Distribution by Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Distance')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

#Remove distance_94 outliers using the same approach as for voyage sea duration
Bulk_PA = remove_outliers_by_region(Bulk_PA, 'distance_94', 'region_pairs')

# Create the input for the box plot analysis (analysis of all outliers removal)
# Create a copy of the relevant columns from Bulk_PA
flagged_outliers_3 = Bulk_PA[['imo', 'load_end_date', 'discharge_start_date', 'voy_sea_duration_h', 'distance', 'distance_94']].copy()

# Get unique region pairs
unique_region_pairs = Bulk_PA['region_pairs'].unique()

# Create binary columns for each region pair
for region_pair in unique_region_pairs:

    flagged_outliers_3[region_pair] = (Bulk_PA['region_pairs'] == region_pair).astype(int)

# Voyage sea duration box plot for each region pair after voyage sea duration and distance_94 outliers removal
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers_3.columns[5:]

# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers_3.melt(
    id_vars=['voy_sea_duration_h'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='voy_sea_duration_h', data=filtered_data, order=sorted_region_pairs)
plt.title('Voyage Sea Duration Distribution by Region Pair After All Outliers Removal')
plt.xlabel('Region Pair')
plt.ylabel('Voyage sea duration (h)')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

# Distance_94 box plot for each region pair after voyage sea duration and distance_94 outliers removal
# Get unique region pairs (excluding the initial columns)
region_pair_columns = flagged_outliers_3.columns[5:]
# Melt the DataFrame to long format for Seaborn
melted_data = flagged_outliers_3.melt(
    id_vars=['distance_94'],
    value_vars=region_pair_columns,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_data = melted_data[melted_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='distance_94', data=filtered_data, order=sorted_region_pairs)
plt.title('Distance_94 Distribution by Region Pair After All Outliers Removal')
plt.xlabel('Region Pair')
plt.ylabel('Distance')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

"""All the outliers have now been removed!"""

# Final distribution of voyage duration, and voyage sea duration after filtering the outliers
# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # 2 row, 2 columns

# Boxplot for voy_duration_h
sns.boxplot(x=Bulk_PA['voy_duration_h'], ax=axes[0, 0])
axes[0, 0].set_title('Box Plot of Voyage Duration (h)')
axes[0, 0].set_xlabel('Voyage Duration (h)')

# Boxplot for voy_sea_duration_h
sns.boxplot(x=Bulk_PA['voy_sea_duration_h'], ax=axes[0, 1])
axes[0, 1].set_title('Box Plot of Voyage Sea Duration (h)')
axes[0, 1].set_xlabel('Voyage Sea Duration (h)')

# Boxplot for distance_94
sns.boxplot(x=Bulk_PA['distance_94'], ax=axes[1, 0])
axes[1, 0].set_title('Box Plot of distance_94')
axes[1, 0].set_xlabel('Distance in nm')

plt.tight_layout()
plt.show()

# Voyage Sea Duration Mean
# Calculate the mean of the 'voy_sea_duration_h' column
mean_voy_sea_duration = Bulk_PA['voy_sea_duration_h'].mean()

# Print the calculated mean
print(f"The mean of voy_sea_duration_h in Bulk_PA is: {mean_voy_sea_duration}")

#Fuel consumption Distribution:
# Create a copy of the relevant columns from Bulk_PA
fuel_region_plot = Bulk_PA[['imo', 'load_end_date', 'discharge_start_date', 'fuel_consumption_mt']].copy()

# Get unique region pairs
unique_region_pairs = Bulk_PA['region_pairs'].unique()

# Create binary columns for each region pair
for region_pair in unique_region_pairs:
    fuel_region_plot[region_pair] = (Bulk_PA['region_pairs'] == region_pair).astype(int)

# Get unique region pairs (excluding the initial columns)
region_pair_columns_fuel = fuel_region_plot.columns[4:]

# Melt the DataFrame to long format for Seaborn
melted_fuel_data = fuel_region_plot.melt(
    id_vars=['fuel_consumption_mt'],
    value_vars=region_pair_columns_fuel,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_fuel_data = melted_fuel_data[melted_fuel_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_fuel_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='fuel_consumption_mt', data=filtered_fuel_data, order=sorted_region_pairs)
plt.title('Distribution of Fuel Consumption per Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Fuel consumption (mt)')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

"""As we can see we have one value of fuel consumption that is a clear outlier. We will remove the outliers"""

# Filter the DataFrame to show rows identify the fuel_consumption_mt that is greater than 50000
high_fuel_consumption = Bulk_PA[Bulk_PA['fuel_consumption_mt'] >= 50000]

# Display the filtered DataFrame
display(high_fuel_consumption)

# Remove fuel consumption outliers
def remove_outliers_by_region(df, column_name, region_column, multiplier=1.5):
    """Removes outliers from a DataFrame based on IQR for each region.

    Args:
        df: The DataFrame to modify.
        column_name: The name of the column containing the data to check for outliers.
        region_column: The name of the column containing the region information.
        multiplier: The IQR multiplier to define outliers (default: 1.5).

    Returns:
        The modified DataFrame with outliers removed.
    """

    filtered_df = df.copy()

    for region in df[region_column].unique():
        region_data = df[df[region_column] == region][column_name]
        Q1 = region_data.quantile(0.25)
        Q3 = region_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - multiplier * IQR
        upper_bound = Q3 + multiplier * IQR

        # Filter out outliers for the current region
        filtered_df = filtered_df[
            ~(
                (filtered_df[region_column] == region)
                & ((filtered_df[column_name] < lower_bound) | (filtered_df[column_name] > upper_bound))
            )
        ]

    # Reset the index of the filtered DataFrame
    filtered_df = filtered_df.reset_index(drop=True)

    return filtered_df

# Apply the function to remove outliers from 'fuel_consumption_mt'
Bulk_PA = remove_outliers_by_region(Bulk_PA, 'fuel_consumption_mt', 'region_pairs')

# Distribution of fuel consumption per region pair
# Create a copy of the relevant columns from Bulk_PA
fuel_region_plot = Bulk_PA[['imo', 'load_end_date', 'discharge_start_date', 'fuel_consumption_mt']].copy()

# Get unique region pairs
unique_region_pairs = Bulk_PA['region_pairs'].unique()

# Create binary columns for each region pair
for region_pair in unique_region_pairs:
    fuel_region_plot[region_pair] = (Bulk_PA['region_pairs'] == region_pair).astype(int)

# Get unique region pairs (excluding the initial columns)
region_pair_columns_fuel = fuel_region_plot.columns[4:]

# Melt the DataFrame to long format for Seaborn
melted_fuel_data = fuel_region_plot.melt(
    id_vars=['fuel_consumption_mt'],
    value_vars=region_pair_columns_fuel,
    var_name='region_pair',
    value_name='is_region_pair'
)

# Filter data to keep only rows where is_region_pair is 1
filtered_fuel_data = melted_fuel_data[melted_fuel_data['is_region_pair'] == 1]

# Sort the unique region pairs alphabetically
sorted_region_pairs = sorted(filtered_fuel_data['region_pair'].unique())

# Create the box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='region_pair', y='fuel_consumption_mt', data=filtered_fuel_data, order=sorted_region_pairs)
plt.title('Distribution of Fuel Consumption per Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Fuel consumption (mt)')
plt.xticks(rotation=60)
plt.tight_layout()
plt.show()

"""### **Second shortest distance**"""

# Input for the analysis to determine the second shortest diatance (rerouted bulk carriers)
# List the top 3 ports for each of the OD_region in terms of transit number based on load_port

# Group by OD region and port and count the number of transits (rows)
Bulk_PA_pre_EL_Nino = Bulk_PA[Bulk_PA['load_end_date']<'2023-06']
port_transit_counts = Bulk_PA_pre_EL_Nino.groupby(['OD_region', 'load_port'])['load_draft'].count().reset_index(name='transit_count')

# Sort within each region and get the top 3
top_ports_per_region = port_transit_counts.groupby('OD_region').apply(lambda x: x.nlargest(3, 'transit_count')).reset_index(drop=True)

# Add a frequency column to display "Transit_count/Total_transits_in_that_OD_Region"
total_transits_per_region = port_transit_counts.groupby('OD_region')['transit_count'].sum().reset_index(name='total_transits')

# Print the result
display(top_ports_per_region)
display(total_transits_per_region)

# Merge the two dataframes
merged_ports_info = pd.merge(top_ports_per_region, total_transits_per_region, on='OD_region')

# Calculate the frequency in percentage
merged_ports_info['frequency_%'] = (merged_ports_info['transit_count'] / merged_ports_info['total_transits']) * 100

# Display the combined information
display(merged_ports_info)

# Set pandas options to display all column content
pd.set_option('display.max_colwidth', None)

# Merge the two dataframes
merged_ports_info = pd.merge(top_ports_per_region, total_transits_per_region, on='OD_region')

# Calculate the frequency in percentage
merged_ports_info['frequency_%'] = (merged_ports_info['transit_count'] / merged_ports_info['total_transits']) * 100

# Create the new table with aggregated information
new_table_aggregated = merged_ports_info.groupby('OD_region').agg(
    load_ports=('load_port', lambda x: list(x)),
    sum_top3_transit_count=('transit_count', 'sum'),
    total_transits=('total_transits', 'first')
).reset_index()

# Calculate the total top 3 frequency
new_table_aggregated['total_top3_frequency_%'] = (new_table_aggregated['sum_top3_transit_count'] / new_table_aggregated['total_transits']) * 100

# Sort the table by 'total_top3_frequency_%' in descending order
new_table_sorted = new_table_aggregated.sort_values(by='total_transits', ascending=False).reset_index(drop=True)

# Format the 'total_top3_frequency_%' column to 2 decimal places
new_table_sorted['total_top3_frequency_%'] = new_table_sorted['total_top3_frequency_%'].round(2)

# Define a dictionary for renaming columns
new_column_names = {
    'OD_region': 'O-D Region',
    'load_ports': 'Top 3 Load Ports',
    'sum_top3_transit_count': 'Total Transits (Top 3 Ports)',
    'total_transits': 'Total Transits in O-D Region',
    'total_top3_frequency_%': 'Frequency of Top 3 Ports (%)'
}

# Rename the columns
new_table_sorted = new_table_sorted.rename(columns=new_column_names)

# Display the sorted and formatted table with new column names
display(new_table_sorted)

# Further analysis are presented in the Excel sheet
# Import the second shortest distance from Excel
second_shortest_distance = pd.read_excel('Second shortest distances.xlsx',sheet_name='Distance2nd')

second_shortest_distance

Bulk_PA.head()

# Create new columns in Bulk_PA to include the information from second_shortest_distance data frame
Bulk_PA['shortest_distance_nm'] = np.nan
Bulk_PA['shortest_distance_day'] = np.nan
Bulk_PA['second_shortest_distance_nm'] = np.nan
Bulk_PA['second_shortest_distance_day'] = np.nan
Bulk_PA['rerouting_nm'] = np.nan
Bulk_PA['rerouting_day'] = np.nan
Bulk_PA['rerouting (%)'] = np.nan
Bulk_PA['route'] = np.nan


# Match region_pairs in Bulk_PA to Region_pair in Second_shortest_distances
# and fill the new columns with the corresponding values
for index, row in Bulk_PA.iterrows():
    region_pair = row['region_pairs']
    match = second_shortest_distance[second_shortest_distance['Region_pair'] == region_pair]
    if not match.empty:
        Bulk_PA.loc[index, 'shortest_distance_nm'] = match.iloc[0]['Shortest_distance_nm']
        Bulk_PA.loc[index, 'shortest_distance_day'] = match.iloc[0]['Shortest_distance_day']
        Bulk_PA.loc[index, 'second_shortest_distance_nm'] = match.iloc[0]['Second_shortest_distance_nm']
        Bulk_PA.loc[index, 'second_shortest_distance_day'] = match.iloc[0]['Second_shortest_distance_day']
        Bulk_PA.loc[index, 'rerouting_nm'] = match.iloc[0]['Rerouting_nm']
        Bulk_PA.loc[index, 'rerouting_day'] = match.iloc[0]['Rerouting_day']
        Bulk_PA.loc[index, 'rerouting (%)'] = match.iloc[0]['Rerouting (%)']
        Bulk_PA.loc[index, 'route'] = match.iloc[0]['Route']
# Display the updated Bulk_PA DataFrame to see the new columns
display(Bulk_PA)

# Voyage Sea Duration vs Second Shortest Distance
# Prepare the data for plotting
melted_duration_distance = Bulk_PA.melt(
    id_vars=['region_pairs'],
    value_vars=['voy_sea_duration_h', 'second_shortest_distance_day'],
    var_name='Metric',
    value_name='Value'
)

# Convert 'voy_sea_duration_h' values to days
melted_duration_distance.loc[melted_duration_distance['Metric'] == 'voy_sea_duration_h', 'Value'] = melted_duration_distance['Value'] / 24
# Rename the 'Metric' for the converted duration for clarity in the legend
melted_duration_distance['Metric'] = melted_duration_distance['Metric'].replace({'voy_sea_duration_h': 'voy_sea_duration_days'})


# Sort the unique region pairs alphabetically for consistent plotting
sorted_region_pairs = sorted(melted_duration_distance['region_pairs'].unique())

# Create the box plot
plt.figure(figsize=(18, 8)) # Adjust figure size as needed
sns.boxplot(x='region_pairs', y='Value', hue='Metric', data=melted_duration_distance, order=sorted_region_pairs)

plt.title('Voyage Sea Duration vs. Second Shortest Distance by Region Pair')
plt.xlabel('Region Pair')
plt.ylabel('Duration (Days)') # Updated label to reflect days
plt.xticks(rotation=60)
plt.legend(title='Metric')
plt.tight_layout()
plt.show()

# Create a table comparing shortest and second shortest distance
# Group by 'region_pairs' and calculate the mean of the specified columns and the count of transits
rerouting_analysis_df = Bulk_PA.groupby('region_pairs').agg(
    transit_count=('region_pairs', 'count'),
    average_distance=('distance_94', 'mean'),
    average_shortest_distance_nm=('shortest_distance_nm', 'mean'),
    average_second_shortest_distance_nm=('second_shortest_distance_nm', 'mean'),
    average_rerouting_percentage=('rerouting (%)', 'mean')
).reset_index()

# Format the mean columns by rounding to 0 decimal places
rerouting_analysis_df['average_distance'] = rerouting_analysis_df['average_distance'].round(0)
rerouting_analysis_df['average_shortest_distance_nm'] = rerouting_analysis_df['average_shortest_distance_nm'].round(0)
rerouting_analysis_df['average_second_shortest_distance_nm'] = rerouting_analysis_df['average_second_shortest_distance_nm'].round(0)
rerouting_analysis_df['average_rerouting_percentage'] = rerouting_analysis_df['average_rerouting_percentage'].round(0)

# Convert the columns to integer type to remove decimal display
rerouting_analysis_df['average_distance'] = rerouting_analysis_df['average_distance'].astype(int)
rerouting_analysis_df['average_shortest_distance_nm'] = rerouting_analysis_df['average_shortest_distance_nm'].astype(int)
rerouting_analysis_df['average_second_shortest_distance_nm'] = rerouting_analysis_df['average_second_shortest_distance_nm'].astype(int)
rerouting_analysis_df['average_rerouting_percentage'] = rerouting_analysis_df['average_rerouting_percentage'].astype(int)

# Merge the second shortest distance routes from the Excel file
rerouting_analysis_df = rerouting_analysis_df.merge(
    second_shortest_distance[['Region_pair', 'Route']],
    left_on='region_pairs',
    right_on='Region_pair',
    how='left'
)
# Drop the redundant 'Region_pair' column from the merge
rerouting_analysis_df = rerouting_analysis_df.drop(columns=['Region_pair'])

# Filter 'distance_increae' in ascending order
rerouting_analysis_df = rerouting_analysis_df.sort_values(by='average_rerouting_percentage', ascending=False)

# Rename the columns after aggregation to include spaces
rerouting_analysis_df = rerouting_analysis_df.rename(columns={
    'average_distance': 'average distance (nm)',
    'average_shortest_distance_nm': 'average shortest distance (nm)',
    'average_second_shortest_distance_nm': 'average second shortest distance (nm)',
    'average_rerouting_percentage': 'distance increase (%)',
    'transit_count': 'transit count',
    'Route': 'second shortest route'
})
# Reset index
rerouting_analysis_df = rerouting_analysis_df.reset_index(drop=True)
# Start index from 1
rerouting_analysis_df.index = rerouting_analysis_df.index + 1

# Display the resulting DataFrame
display(rerouting_analysis_df)

"""### **Graphs and Analytics**

### **Limitations**

There are still a lot of distances that are not coherent with the origin-destination of the transit. Adjusting these values would be too subjective or too strict. We will work with these limitations.
"""

# The most problematic regions are associated to Europe_N_Africa-NW_America and Europe_N_Africa-SW-America as has been shown previously from the distance 94 box plot
# Example of outliers (small values)
# Filter by region_pairs and sort by distance_94
Europe_N_Africa_NW_America_data = Bulk_PA[Bulk_PA['region_pairs'] == 'Europe_N_Africa-NW_America'].sort_values(by=['distance_94']).reset_index(drop=True)

Europe_N_Africa_NW_America_data.head(10)

# Histogram of Distance_94 between Europe-North Africa and the West of North America
plt.hist(Europe_N_Africa_NW_America_data['distance_94'])
plt.xlabel('Distance_94')
plt.ylabel('Frequency')
plt.title('Histogram of Distance_94 between Europe-North Africa and North West America')
plt.show()

"""The mean of the distribution is the distance expected. However, the minimum value is really small when compared to the expected distance between Europe_North_Africa and North West America. These voyages should average around 8000nm as estimated from dataloy."""

# Download the cleaned dataset
Bulk_PA.to_parquet('Bulk_PA_Clean.parquet')

"""From now on, we will use this dataset for further analysis

## **Deleted Rows analysis**

Compare the observations before the filtering and after the filtering to know how strict is the filtering
"""

# Remove duplicates based on imo, load_end_date, and discharge_start_date â€” keep the first occurrence
Bulk_PA_original_data = Bulk_PA_original_data.drop_duplicates(subset=['vsl_imo', 'load_end_date', 'discharge_start_date'], keep='first').reset_index(drop=True)

print("Duplicates removed. Only one entry per unique (imo, load_end_date, discharge_start_date) remains.")
display(Bulk_PA_original_data.shape)

# Merge the DataFrames (original and filtered data) and indicate the source of each row
merged_df = pd.merge(Bulk_PA_original_data, Bulk_PA,
                     on=['load_end_date', 'discharge_start_date'],
                     how='left', indicator=True)

# Filter for rows present only in the original data
missing_rows = merged_df[merged_df['_merge'] == 'left_only']

# Display the missing rows
display(missing_rows)

# Convert 'load_end_date' to datetime if it's not already
missing_rows['load_end_date'] = pd.to_datetime(missing_rows['load_end_date'])

# Create the histogram
plt.figure(figsize=(10, 6))
plt.hist(missing_rows['load_end_date'], bins=20)
plt.title('Distribution of Load End Dates for Deleted Rows')
plt.xlabel('Load End Date')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""As expected, a big proportion of the filtered values are from the time frame filtering - we want to keep observations within the 2019-2024 time frame. Otherwise, we have deleted a fairly uniform number of transits throughout the period."""

# Create a copy of Bulk_PA
Bulk_PA_copy = Bulk_PA.copy()

# Create a month column based on load_end_date for both df
Bulk_PA_copy['month'] = Bulk_PA_copy['load_end_date'].dt.to_period('M')
Bulk_PA_original_data['month'] = Bulk_PA_original_data['load_end_date'].dt.to_period('M')

# Group by load_end_date and count occurrences for Bulk_PA
bulk_pa_counts = Bulk_PA_copy.groupby('month')['month'].count().reset_index(name='count')

# Group by load_end_date and count occurrences for Bulk_PA_original_data
bulk_pa_original_counts = Bulk_PA_original_data.groupby('month')['month'].count().reset_index(name='count')

# Convert 'month' to timestamp before concatenating
bulk_pa_counts['month'] = bulk_pa_counts['month'].dt.to_timestamp()
bulk_pa_original_counts['month'] = bulk_pa_original_counts['month'].dt.to_timestamp()

# Combine the two DataFrames
combined_counts = pd.concat([
    bulk_pa_counts.assign(dataset='Bulk_PA_copy'),
    bulk_pa_original_counts.assign(dataset='Bulk_PA_original_data_copy')
])

# Create the plot
plt.figure(figsize=(12, 6))
sns.lineplot(x='month', y='count', hue='dataset', data=combined_counts)
plt.title('Comparison of Monthly Voyages Pre and Post Filtering')
plt.xlabel('Month')
plt.ylabel('Number of voyages')
plt.legend(title='Dataset')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()